{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory applied machine learning (INFR10069)\n",
    "# Assignment 3 (Part B): Mini-Challenge [25%]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Instructions\n",
    "\n",
    "**It is important that you follow the instructions below to the letter - we will not be responsible for incorrect marking due to non-standard practices.**\n",
    "\n",
    "1. <font color='red'>We have split Assignment 3 into two parts to make it easier for you to work on them separately and for the markers to give you feedback. This is part B of Assignment 3 - Part A is an introduction to Object Recognition. Both Assignments together are still worth 50% of CourseWork 2. **Remember to submit both notebooks (you can submit them separately).**</font>\n",
    "\n",
    "1. You *MUST* have your environment set up as in the [README](https://github.com/michael-camilleri/IAML2018) and you *must activate this environment before running this notebook*:\n",
    "```\n",
    "source activate py3iaml\n",
    "cd [DIRECTORY CONTAINING GIT REPOSITORY]\n",
    "jupyter notebook\n",
    "# Navigate to this file\n",
    "```\n",
    "\n",
    "1. Read the instructions carefully, especially where asked to name variables with a specific name. Wherever you are required to produce code you should use code cells, otherwise you should use markdown cells to report results and explain answers. In most cases we indicate the nature of answer we are expecting (code/text), and also provide the code/markdown cell where to put it\n",
    "\n",
    "1. This part of the Assignment is the same for all students i.e. irrespective of whether you are taking the Level 10 version (INFR10069) or the Level-11 version of the course (INFR11182 and INFR11152).\n",
    "\n",
    "1. The .csv files that you will be using are located at `./datasets` (i.e. use the `datasets` directory **adjacent** to this file).\n",
    "\n",
    "1. In the textual answer, you are given a word-count limit of 600 words: exceeding this will lead to penalisation.\n",
    "\n",
    "1. Make sure to distinguish between **attributes** (columns of the data) and **features** (which typically refers only to the independent variables, i.e. excluding the target variables).\n",
    "\n",
    "1. Make sure to show **all** your code/working. \n",
    "\n",
    "1. Write readable code. While we do not expect you to follow [PEP8](https://www.python.org/dev/peps/pep-0008/) to the letter, the code should be adequately understandable, with plots/visualisations correctly labelled. **Do** use inline comments when doing something non-standard. When asked to present numerical values, make sure to represent real numbers in the appropriate precision to exemplify your answer. Marks *WILL* be deducted if the marker cannot understand your logic/results.\n",
    "\n",
    "1. **Collaboration:** You may discuss the assignment with your colleagues, provided that the writing that you submit is entirely your own. That is, you must NOT borrow actual text or code from others. We ask that you provide a list of the people who you've had discussions with (if any). Please refer to the [Academic Misconduct](http://web.inf.ed.ac.uk/infweb/admin/policies/academic-misconduct) page for what consistutes a breach of the above.\n",
    "\n",
    "\n",
    "### SUBMISSION Mechanics\n",
    "\n",
    "**IMPORTANT:** You must submit this assignment by **Thursday 15/11/2018 at 16:00**. \n",
    "\n",
    "**Late submissions:** The policy stated in the School of Informatics is that normally you will not be allowed to submit coursework late. See the [ITO webpage](http://web.inf.ed.ac.uk/infweb/student-services/ito/admin/coursework-projects/late-coursework-extension-requests) for exceptions to this, e.g. in case of serious medical illness or serious personal problems.\n",
    "\n",
    "**Resubmission:** If you submit your file(s) again, the previous submission is **overwritten**. We will mark the version that is in the submission folder at the deadline.\n",
    "\n",
    "**N.B.**: This Assignment requires submitting **two files (electronically as described below)**:\n",
    " 1. This Jupyter Notebook (Part B), *and*\n",
    " 1. The Jupyter Notebook for Part A\n",
    " \n",
    "All submissions happen electronically. To submit:\n",
    "\n",
    "1. Fill out this notebook (as well as Part A), making sure to:\n",
    "   1. save it with **all code/text and visualisations**: markers are NOT expected to run any cells,\n",
    "   1. keep the name of the file **UNCHANGED**, *and*\n",
    "   1. **keep the same structure**: retain the questions, **DO NOT** delete any cells and **avoid** adding unnecessary cells unless absolutely necessary, as this makes the job harder for the markers. This is especially important for the textual description and probability output (below).\n",
    "\n",
    "1. Submit it using the `submit` functionality. To do this, you must be on a DICE environment. Open a Terminal, and:\n",
    "   1. **On-Campus Students**: navigate to the location of this notebook and execute the following command:\n",
    "   \n",
    "      ```submit iaml cw2 03_A_ObjectRecognition.ipynb 03_B_MiniChallenge.ipynb```\n",
    "      \n",
    "   1. **Distance Learners:** These instructions also apply to those students who work on their own computer. First you need to copy your work onto DICE (so that you can use the `submit` command). For this, you can use `scp` or `rsync` (you may need to install these yourself). You can copy files to `student.ssh.inf.ed.ac.uk`, then ssh into it in order to submit. The following is an example. Replace entries in `[square brackets]` with your specific details: i.e. if your student number is for example s1234567, then `[YOUR USERNAME]` becomes `s1234567`.\n",
    "   \n",
    "    ```\n",
    "    scp -r [FULL PATH TO 03_A_ObjectRecognition.ipynb] [YOUR USERNAME]@student.ssh.inf.ed.ac.uk:03_A_ObjectRecognition.ipynb\n",
    "    scp -r [FULL PATH TO 03_B_MiniChallenge.ipynb] [YOUR USERNAME]@student.ssh.inf.ed.ac.uk:03_B_MiniChallenge.ipynb\n",
    "    ssh [YOUR USERNAME]@student.ssh.inf.ed.ac.uk\n",
    "    ssh student.login\n",
    "    submit iaml cw2 03_A_ObjectRecognition.ipynb 03_B_MiniChallenge.ipynb\n",
    "    ```\n",
    "    \n",
    "   What actually happens in the background is that your file is placed in a folder available to markers. If you submit a file with the same name into the same location, **it will *overwrite* your previous submission**. You should receive an automatic email confirmation after submission.\n",
    "  \n",
    "\n",
    "\n",
    "### Marking Breakdown\n",
    "\n",
    "The Level 10 and Level 11 points are marked out of different totals, however these are all normalised to 100%. Note that Part A (Object Recognition) is worth 75% of the total Mark for Assignment 3, while Part B (this notebook) is worth 25%. Keep this in mind when allocating time for this assignment.\n",
    "\n",
    "**70-100%** results/answer correct plus extra achievement at understanding or analysis of results. Clear explanations, evidence of creative or deeper thought will contribute to a higher grade.\n",
    "\n",
    "**60-69%** results/answer correct or nearly correct and well explained.\n",
    "\n",
    "**50-59%** results/answer in right direction but significant errors.\n",
    "\n",
    "**40-49%** some evidence that the student has gained some understanding, but not answered the questions\n",
    "properly.\n",
    "\n",
    "**0-39%** serious error or slack work.\n",
    "\n",
    "Note that while this is not a programming assignment, in questions which involve visualisation of results and/or long cold snippets, some marks may be deducted if the code is not adequately readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Use the cell below to include any imports you deem necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zerosum24/.conda/envs/py3iaml/lib/python3.7/site-packages/sklearn/utils/__init__.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence\n"
     ]
    }
   ],
   "source": [
    "# Nice Formatting within Jupyter Notebook\n",
    "%matplotlib inline\n",
    "from IPython.display import display # Allows multiple displays from a single code-cell\n",
    "\n",
    "# System functionality\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import Here any Additional modules you use. To import utilities we provide, use something like:\n",
    "#   from utils.plotter import plot_hinton\n",
    "\n",
    "# Your Code goes here:\n",
    "\n",
    "\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss, r2_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "KNeighboursClassifier = KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini challenge\n",
    "\n",
    "In this second part of the assignment we will have a mini object-recognition challenge. Using the same type of data as in Part A, you are asked to find the best classifier for the person/no person classification task. You can apply any preprocessing steps to the data that you think fit and employ any classifier you like (with the provision that you can explain what the classifier is/preprocessing steps are doing). You can also employ any lessons learnt during the course, either from previous Assignments, the Labs or the lecture material to try and squeeze out as much performance as you possibly can. The only restriction is that all steps must be performed in `Python` by using the `numpy`, `pandas` and `sklearn` packages. You can also make use of `matplotlib` and `seaborn` for visualisation.\n",
    "\n",
    "### DataSet Description\n",
    "\n",
    "The datasets we use here are similar in composition but not the same as the ones used in Part A: *it will be useful to revise the description in that notebook*. Specifically, you have access to three new datasets: a training set (`Images_C_Train.csv`), a validation set (`Images_C_Validate.csv`), and a test set (`Images_C_Test.csv`). You must use the former two for training and evaluating your models (as you see fit). As before, the full data-set has 520 attributes (dimensions). Of these you only have access to the 500 features (`dim1` through `dim500`) to test your model on: i.e. the test set does not have any of the class labels.\n",
    "\n",
    "### Model Evaluation\n",
    "\n",
    "Your results will be evaluated in terms of the logarithmic loss metric, specifically the [logloss](http://scikit-learn.org/0.19/modules/model_evaluation.html#log-loss) function from SKLearn. You should familiarise yourself with this. To estimate this metric you will need to provide probability outputs, as opposed to discrete predictions which we have used so far to compute classification accuracies. Most models in `sklearn` implement a `predict_proba()` method which returns the probabilities for each class. For instance, if your test set consists of `N` datapoints and there are `K` class-labels, the method will return an `N` x `K` matrix (with rows summing to 1).\n",
    "\n",
    "### Submission and Scoring\n",
    "\n",
    "This part of Assignment 3 carries 25% of the total marks. Within this, you will be scored on two criteria:\n",
    " 1. 80% of the mark will depend on the thoroughness of the exploration of various approaches. This will be assessed through your code, as well as a brief description (<600 words) justifying the approaches you considered, your exploration pattern and your suggested final approach (and why you chose it).\n",
    " 1. 20% of the mark will depend on the quality of your predictions: this will be evaluated based on the logarithmic loss metric.\n",
    "Note here that just getting exceptional performance is not enough: in fact, you should focus more on analysing your results that just getting the best score!\n",
    "\n",
    "You have to submit the following:\n",
    " 1. **All Code-Cells** which show your **working** with necessary output/plots already generated.\n",
    " 1. In **TEXT** cell `#ANSWER_TEXT#` you are to write your explanation (<600 words) as described above. Keep this brief and to the point. **Make sure** to keep the token `#ANSWER_TEXT#` as the first line of the cell!\n",
    " 1. In **CODE** cell `#ANSWER_PROB#` you are to submit your predictions. To do this:\n",
    "    1. Once you have chosen your favourite model (and pre-processing steps) apply it to the test-set and estimate the posterior proabilities for the data points in the test set.\n",
    "    1. Store these probabilities in a 2D numpy array named `pred_probabilities`, with predictions along the rows i.e. each row should be a complete probability distribution over whether the image contains a person or not. Note that due to the encoding of the `is_person` class, the negative case (i.e. there is no person) comes first.\n",
    "    1. Execute the `#ANSWER_PROB#` code cell, making sure to not change anything. This cell will do some checks to ensure that you are submitting the right shape of array.\n",
    "\n",
    "You may create as many code cells as you need (within reason) for training your models, evaluating the data etc: however, the text cell `#ANSWER_TEXT#` and code-cell `#ANSWER_PROB#` showing your answers must be the last two cells in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where your working code should start. Fell free to add as many code-cells as necessary.\n",
    "#  Make sure however that all working code cells come BEFORE the #ANSWER_TEXT# and #ANSWER_PROB#\n",
    "#  cells below.\n",
    "\n",
    "# Your Code goes here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:   The number of entries is 2113 and the amount of attributes is 520.\n",
      "Validation: The number of entries is 1113 and the amount of attributes is 520.\n",
      "Testing:    The number of entries is 1114 and the amount of attributes is 501: with none of the class labels\n",
      "            (barring a null valued is_person column).\n"
     ]
    }
   ],
   "source": [
    "# Loading the training/testing datasets for Images_C_*.csv\n",
    "\n",
    "# Loading the training set\n",
    "data_path = os.path.join(os.getcwd(), 'datasets', 'Images_C_Train.csv')\n",
    "images_c_train = pd.read_csv(data_path)\n",
    "\n",
    "# Loading the testing set\n",
    "data_path = os.path.join(os.getcwd(), 'datasets', 'Images_C_Test.csv')\n",
    "images_c_test = pd.read_csv(data_path)\n",
    "\n",
    "# Loading the validation set\n",
    "data_path = os.path.join(os.getcwd(), 'datasets', 'Images_C_Validate.csv')\n",
    "images_c_valid = pd.read_csv(data_path)\n",
    "\n",
    "print(\"Training:   The number of entries is {0} and the amount of attributes is {1}.\"\n",
    "      .format(images_c_train.shape[0], images_c_train.shape[1]))\n",
    "print(\"Validation: The number of entries is {0} and the amount of attributes is {1}.\"\n",
    "      .format(images_c_valid.shape[0], images_c_valid.shape[1]))\n",
    "print(\"Testing:    The number of entries is {0} and the amount of attributes is {1}: with none of the class labels\"\n",
    "      .format(images_c_test.shape[0], images_c_test.shape[1]) \n",
    "      + \"\\n            (barring a null valued is_person column).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to filter the unneeded features of a dataframe\n",
    "def feature_filtering(dataframe, regex=\"dim\"):\n",
    "\n",
    "    # last data structure to hold unwanted features\n",
    "    indx_lst = []\n",
    "\n",
    "    # Building a list of any features that are not visual features\n",
    "    for attribute in dataframe.columns:\n",
    "        # ensuring attribute is not a visual feature\n",
    "        if attribute[0:len(regex)] != regex:\n",
    "            indx_lst.append(attribute)\n",
    "            \n",
    "    return dataframe.drop(columns=indx_lst, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into training, testing and validation sets\n",
    "\n",
    "# Splitting the training set\n",
    "X_train = feature_filtering(images_c_train)\n",
    "y_train = images_c_train['is_person']\n",
    "\n",
    "# Splitting the validation set\n",
    "X_valid = feature_filtering(images_c_valid)\n",
    "y_valid = images_c_valid['is_person'].values\n",
    "\n",
    "# Splitting the testing set\n",
    "X_test = feature_filtering(images_c_test)\n",
    "y_test = images_c_test['is_person'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train properties\n",
      "------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2113 entries, 0 to 2112\n",
      "Columns: 500 entries, dim1 to dim500\n",
      "dtypes: float64(500)\n",
      "memory usage: 8.1 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim1</th>\n",
       "      <th>dim2</th>\n",
       "      <th>dim3</th>\n",
       "      <th>dim4</th>\n",
       "      <th>dim5</th>\n",
       "      <th>dim6</th>\n",
       "      <th>dim7</th>\n",
       "      <th>dim8</th>\n",
       "      <th>dim9</th>\n",
       "      <th>dim10</th>\n",
       "      <th>...</th>\n",
       "      <th>dim491</th>\n",
       "      <th>dim492</th>\n",
       "      <th>dim493</th>\n",
       "      <th>dim494</th>\n",
       "      <th>dim495</th>\n",
       "      <th>dim496</th>\n",
       "      <th>dim497</th>\n",
       "      <th>dim498</th>\n",
       "      <th>dim499</th>\n",
       "      <th>dim500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "      <td>2113.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.043353</td>\n",
       "      <td>0.050830</td>\n",
       "      <td>0.047988</td>\n",
       "      <td>0.037411</td>\n",
       "      <td>0.043365</td>\n",
       "      <td>0.049670</td>\n",
       "      <td>0.051071</td>\n",
       "      <td>0.043601</td>\n",
       "      <td>0.052698</td>\n",
       "      <td>0.054139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050306</td>\n",
       "      <td>0.052482</td>\n",
       "      <td>0.044370</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>0.043218</td>\n",
       "      <td>0.049724</td>\n",
       "      <td>0.050818</td>\n",
       "      <td>0.038214</td>\n",
       "      <td>0.050150</td>\n",
       "      <td>0.051801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.513260</td>\n",
       "      <td>0.582839</td>\n",
       "      <td>0.482166</td>\n",
       "      <td>0.464611</td>\n",
       "      <td>0.491187</td>\n",
       "      <td>0.558422</td>\n",
       "      <td>0.577599</td>\n",
       "      <td>0.472706</td>\n",
       "      <td>0.571629</td>\n",
       "      <td>0.588484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561111</td>\n",
       "      <td>0.616955</td>\n",
       "      <td>0.466802</td>\n",
       "      <td>0.528117</td>\n",
       "      <td>0.508790</td>\n",
       "      <td>0.561206</td>\n",
       "      <td>0.562799</td>\n",
       "      <td>0.437885</td>\n",
       "      <td>0.581028</td>\n",
       "      <td>0.569857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.003516</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.002404</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>0.006454</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.002734</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003005</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.006641</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.004076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.984000</td>\n",
       "      <td>9.122238</td>\n",
       "      <td>7.676800</td>\n",
       "      <td>9.695738</td>\n",
       "      <td>8.762671</td>\n",
       "      <td>9.489078</td>\n",
       "      <td>9.751526</td>\n",
       "      <td>8.691076</td>\n",
       "      <td>9.013933</td>\n",
       "      <td>9.602705</td>\n",
       "      <td>...</td>\n",
       "      <td>9.922150</td>\n",
       "      <td>9.673318</td>\n",
       "      <td>7.375434</td>\n",
       "      <td>9.672255</td>\n",
       "      <td>9.348755</td>\n",
       "      <td>9.299061</td>\n",
       "      <td>9.951019</td>\n",
       "      <td>9.036268</td>\n",
       "      <td>9.963328</td>\n",
       "      <td>9.505755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              dim1         dim2         dim3         dim4         dim5  \\\n",
       "count  2113.000000  2113.000000  2113.000000  2113.000000  2113.000000   \n",
       "mean      0.043353     0.050830     0.047988     0.037411     0.043365   \n",
       "std       0.513260     0.582839     0.482166     0.464611     0.491187   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000868     0.000000     0.001359     0.000781     0.001116   \n",
       "50%       0.001616     0.000340     0.003516     0.001698     0.002038   \n",
       "75%       0.002404     0.001008     0.006454     0.002717     0.003125   \n",
       "max       9.984000     9.122238     7.676800     9.695738     8.762671   \n",
       "\n",
       "              dim6         dim7         dim8         dim9        dim10  \\\n",
       "count  2113.000000  2113.000000  2113.000000  2113.000000  2113.000000   \n",
       "mean      0.049670     0.051071     0.043601     0.052698     0.054139   \n",
       "std       0.558422     0.577599     0.472706     0.571629     0.588484   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.001019     0.000340     0.000651     0.000756     0.001116   \n",
       "50%       0.001860     0.000756     0.001645     0.001698     0.002155   \n",
       "75%       0.003057     0.001488     0.003397     0.002734     0.003736   \n",
       "max       9.489078     9.751526     8.691076     9.013933     9.602705   \n",
       "\n",
       "          ...            dim491       dim492       dim493       dim494  \\\n",
       "count     ...       2113.000000  2113.000000  2113.000000  2113.000000   \n",
       "mean      ...          0.050306     0.052482     0.044370     0.046100   \n",
       "std       ...          0.561111     0.616955     0.466802     0.528117   \n",
       "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...          0.001019     0.000000     0.000679     0.000679   \n",
       "50%       ...          0.001860     0.000000     0.002717     0.001359   \n",
       "75%       ...          0.003005     0.000679     0.006641     0.002232   \n",
       "max       ...          9.922150     9.673318     7.375434     9.672255   \n",
       "\n",
       "            dim495       dim496       dim497       dim498       dim499  \\\n",
       "count  2113.000000  2113.000000  2113.000000  2113.000000  2113.000000   \n",
       "mean      0.043218     0.049724     0.050818     0.038214     0.050150   \n",
       "std       0.508790     0.561206     0.562799     0.437885     0.581028   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.001764     0.001019     0.000744     0.001116     0.001019   \n",
       "50%       0.003125     0.001953     0.002038     0.002038     0.002038   \n",
       "75%       0.004883     0.003057     0.004076     0.003057     0.003397   \n",
       "max       9.348755     9.299061     9.951019     9.036268     9.963328   \n",
       "\n",
       "            dim500  \n",
       "count  2113.000000  \n",
       "mean      0.051801  \n",
       "std       0.569857  \n",
       "min       0.000000  \n",
       "25%       0.001019  \n",
       "50%       0.002268  \n",
       "75%       0.004076  \n",
       "max       9.505755  \n",
       "\n",
       "[8 rows x 500 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_valid properties\n",
      "------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1113 entries, 0 to 1112\n",
      "Columns: 500 entries, dim1 to dim500\n",
      "dtypes: float64(494), int64(6)\n",
      "memory usage: 4.2 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim1</th>\n",
       "      <th>dim2</th>\n",
       "      <th>dim3</th>\n",
       "      <th>dim4</th>\n",
       "      <th>dim5</th>\n",
       "      <th>dim6</th>\n",
       "      <th>dim7</th>\n",
       "      <th>dim8</th>\n",
       "      <th>dim9</th>\n",
       "      <th>dim10</th>\n",
       "      <th>...</th>\n",
       "      <th>dim491</th>\n",
       "      <th>dim492</th>\n",
       "      <th>dim493</th>\n",
       "      <th>dim494</th>\n",
       "      <th>dim495</th>\n",
       "      <th>dim496</th>\n",
       "      <th>dim497</th>\n",
       "      <th>dim498</th>\n",
       "      <th>dim499</th>\n",
       "      <th>dim500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1113.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.004720</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>0.002110</td>\n",
       "      <td>0.001037</td>\n",
       "      <td>0.002529</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.002641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002142</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.004817</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.003611</td>\n",
       "      <td>0.002196</td>\n",
       "      <td>0.002772</td>\n",
       "      <td>0.002239</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>0.003097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.001364</td>\n",
       "      <td>0.003876</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.002736</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>0.005831</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>0.002471</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>0.003182</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.001868</td>\n",
       "      <td>0.002590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.001803</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002734</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.001838</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.007102</td>\n",
       "      <td>0.002734</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.006793</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.003348</td>\n",
       "      <td>0.004092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.007133</td>\n",
       "      <td>0.022135</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.010789</td>\n",
       "      <td>0.005757</td>\n",
       "      <td>0.022396</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.013927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009821</td>\n",
       "      <td>0.012747</td>\n",
       "      <td>0.042026</td>\n",
       "      <td>0.009821</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.013346</td>\n",
       "      <td>0.029225</td>\n",
       "      <td>0.007068</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.026786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              dim1         dim2         dim3         dim4         dim5  \\\n",
       "count  1113.000000  1113.000000  1113.000000  1113.000000  1113.000000   \n",
       "mean      0.001744     0.000702     0.004720     0.001978     0.002321   \n",
       "std       0.001209     0.001364     0.003876     0.001417     0.001558   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000744     0.000000     0.001698     0.001019     0.001172   \n",
       "50%       0.001563     0.000340     0.003736     0.001698     0.002038   \n",
       "75%       0.002378     0.000758     0.007102     0.002734     0.003057   \n",
       "max       0.007133     0.022135     0.023438     0.008929     0.010417   \n",
       "\n",
       "              dim6         dim7         dim8         dim9        dim10  \\\n",
       "count  1113.000000  1113.000000  1113.000000  1113.000000  1113.000000   \n",
       "mean      0.002110     0.001037     0.002529     0.002006     0.002641   \n",
       "std       0.001559     0.000885     0.002736     0.001919     0.002293   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.001019     0.000340     0.000679     0.000781     0.001019   \n",
       "50%       0.001803     0.000781     0.001698     0.001563     0.002038   \n",
       "75%       0.002976     0.001488     0.003397     0.002717     0.003397   \n",
       "max       0.010789     0.005757     0.022396     0.025000     0.013927   \n",
       "\n",
       "          ...            dim491       dim492       dim493       dim494  \\\n",
       "count     ...       1113.000000  1113.000000  1113.000000  1113.000000   \n",
       "mean      ...          0.002142     0.000598     0.004817     0.001585   \n",
       "std       ...          0.001600     0.001241     0.005831     0.001243   \n",
       "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...          0.001019     0.000000     0.000679     0.000679   \n",
       "50%       ...          0.001838     0.000000     0.002734     0.001359   \n",
       "75%       ...          0.002976     0.000679     0.006793     0.002232   \n",
       "max       ...          0.009821     0.012747     0.042026     0.009821   \n",
       "\n",
       "            dim495       dim496       dim497       dim498       dim499  \\\n",
       "count  1113.000000  1113.000000  1113.000000  1113.000000  1113.000000   \n",
       "mean      0.003611     0.002196     0.002772     0.002239     0.002407   \n",
       "std       0.002471     0.001575     0.003182     0.001346     0.001868   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.001860     0.001019     0.000679     0.001250     0.001019   \n",
       "50%       0.003125     0.001860     0.001838     0.002038     0.002038   \n",
       "75%       0.004836     0.003057     0.003736     0.003057     0.003348   \n",
       "max       0.015625     0.013346     0.029225     0.007068     0.011889   \n",
       "\n",
       "            dim500  \n",
       "count  1113.000000  \n",
       "mean      0.003097  \n",
       "std       0.002590  \n",
       "min       0.000000  \n",
       "25%       0.001359  \n",
       "50%       0.002604  \n",
       "75%       0.004092  \n",
       "max       0.026786  \n",
       "\n",
       "[8 rows x 500 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_test properties\n",
      "------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1114 entries, 0 to 1113\n",
      "Columns: 500 entries, dim1 to dim500\n",
      "dtypes: float64(500)\n",
      "memory usage: 4.2 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim1</th>\n",
       "      <th>dim2</th>\n",
       "      <th>dim3</th>\n",
       "      <th>dim4</th>\n",
       "      <th>dim5</th>\n",
       "      <th>dim6</th>\n",
       "      <th>dim7</th>\n",
       "      <th>dim8</th>\n",
       "      <th>dim9</th>\n",
       "      <th>dim10</th>\n",
       "      <th>...</th>\n",
       "      <th>dim491</th>\n",
       "      <th>dim492</th>\n",
       "      <th>dim493</th>\n",
       "      <th>dim494</th>\n",
       "      <th>dim495</th>\n",
       "      <th>dim496</th>\n",
       "      <th>dim497</th>\n",
       "      <th>dim498</th>\n",
       "      <th>dim499</th>\n",
       "      <th>dim500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.004379</td>\n",
       "      <td>0.001839</td>\n",
       "      <td>0.002207</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.001058</td>\n",
       "      <td>0.002535</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>0.002746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002033</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.004880</td>\n",
       "      <td>0.001537</td>\n",
       "      <td>0.003591</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.002438</td>\n",
       "      <td>0.003017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.001226</td>\n",
       "      <td>0.001522</td>\n",
       "      <td>0.003648</td>\n",
       "      <td>0.001320</td>\n",
       "      <td>0.001539</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>0.002470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.006427</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.002626</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>0.001449</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>0.002795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.001590</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.000822</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.002115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.001813</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.006454</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.003720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.006793</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.004755</td>\n",
       "      <td>0.002734</td>\n",
       "      <td>0.003831</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.003463</td>\n",
       "      <td>0.004076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.009046</td>\n",
       "      <td>0.027699</td>\n",
       "      <td>0.021399</td>\n",
       "      <td>0.007473</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.004755</td>\n",
       "      <td>0.021399</td>\n",
       "      <td>0.021140</td>\n",
       "      <td>0.022500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015253</td>\n",
       "      <td>0.010691</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.007576</td>\n",
       "      <td>0.026834</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>0.029212</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.012228</td>\n",
       "      <td>0.028193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              dim1         dim2         dim3         dim4         dim5  \\\n",
       "count  1114.000000  1114.000000  1114.000000  1114.000000  1114.000000   \n",
       "mean      0.001754     0.000717     0.004379     0.001839     0.002207   \n",
       "std       0.001226     0.001522     0.003648     0.001320     0.001539   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000977     0.000000     0.001488     0.000744     0.001019   \n",
       "50%       0.001590     0.000340     0.003397     0.001698     0.001860   \n",
       "75%       0.002378     0.000781     0.006454     0.002717     0.003057   \n",
       "max       0.009046     0.027699     0.021399     0.007473     0.009766   \n",
       "\n",
       "              dim6         dim7         dim8         dim9        dim10  \\\n",
       "count  1114.000000  1114.000000  1114.000000  1114.000000  1114.000000   \n",
       "mean      0.002105     0.001058     0.002535     0.002053     0.002746   \n",
       "std       0.001612     0.000863     0.002867     0.001990     0.002470   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.001019     0.000340     0.000679     0.000744     0.001065   \n",
       "50%       0.001776     0.000822     0.001596     0.001667     0.002115   \n",
       "75%       0.002717     0.001698     0.003397     0.002717     0.003720   \n",
       "max       0.011889     0.004755     0.021399     0.021140     0.022500   \n",
       "\n",
       "          ...            dim491       dim492       dim493       dim494  \\\n",
       "count     ...       1114.000000  1114.000000  1114.000000  1114.000000   \n",
       "mean      ...          0.002033     0.000577     0.004880     0.001537   \n",
       "std       ...          0.001597     0.001146     0.006427     0.001200   \n",
       "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...          0.000791     0.000000     0.000679     0.000679   \n",
       "50%       ...          0.001698     0.000000     0.002717     0.001359   \n",
       "75%       ...          0.002717     0.000679     0.006793     0.002232   \n",
       "max       ...          0.015253     0.010691     0.062500     0.007576   \n",
       "\n",
       "            dim495       dim496       dim497       dim498       dim499  \\\n",
       "count  1114.000000  1114.000000  1114.000000  1114.000000  1114.000000   \n",
       "mean      0.003591     0.002080     0.002897     0.002216     0.002438   \n",
       "std       0.002626     0.001519     0.003240     0.001449     0.001897   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.001698     0.001019     0.000744     0.001065     0.001019   \n",
       "50%       0.003057     0.001813     0.002038     0.002038     0.002038   \n",
       "75%       0.004755     0.002734     0.003831     0.003057     0.003463   \n",
       "max       0.026834     0.009851     0.029212     0.007812     0.012228   \n",
       "\n",
       "            dim500  \n",
       "count  1114.000000  \n",
       "mean      0.003017  \n",
       "std       0.002795  \n",
       "min       0.000000  \n",
       "25%       0.001019  \n",
       "50%       0.002361  \n",
       "75%       0.004076  \n",
       "max       0.028193  \n",
       "\n",
       "[8 rows x 500 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summarising key properties of the data using the appropriate pandas methods:\n",
    "\n",
    "print(\"X_train properties\")\n",
    "print(\"------------------\")\n",
    "# displaying overview information\n",
    "X_train.info() \n",
    "\n",
    "# displaying statistical information about the dataset\n",
    "display(X_train.describe())\n",
    "\n",
    "print(\"\\nX_valid properties\")\n",
    "print(\"------------------\")\n",
    "# displaying overview information\n",
    "X_valid.info() \n",
    "\n",
    "# displaying statistical information about the dataset\n",
    "display(X_valid.describe())\n",
    "\n",
    "print(\"\\nX_test properties\")\n",
    "print(\"------------------\")\n",
    "# displaying overview information\n",
    "X_test.info() \n",
    "\n",
    "# displaying statistical information about the dataset\n",
    "display(X_test.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1113 entries, 0 to 1112\n",
      "Columns: 500 entries, dim1 to dim500\n",
      "dtypes: float64(500)\n",
      "memory usage: 4.2 MB\n"
     ]
    }
   ],
   "source": [
    "# Based on the overview information, X_valid needs to be scaled \n",
    "# This is because not all of the types are float64\n",
    "\n",
    "X_valid = pd.DataFrame((preprocessing.scale(X_valid)), columns=X_valid.columns) \n",
    "\n",
    "# checking for consistent types\n",
    "X_valid.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfEAAAFNCAYAAAAQOlZzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAHP1JREFUeJzt3X98ZXV95/H3O4lhwhBKGNLRIdgRpdOqDxW5suKsPixoV6vUh30oRcVR151RadW27irYbkFXrd36aHFXpWUQdAAB5ccu26L4e0HbHcyMugjIiggSGCCEqEPIEDP57B/nJM3E/LjJ3Hu/53vzej4e80juveee7+eee+e+c77ne87XESEAAJCfjtQFAACAlSHEAQDIFCEOAECmCHEAADJFiAMAkClCHACATBHiyIbtR20f26R1b7QdtrvK21+0/aYGrfuFtu+Ydftu2y9pxLrL9d1q+8WNWl+dbdr2xbZHbd9cx/JN276tZPubtv9Dncu+2PbQrNstf5/Q/ghxJFfvF2NEHBYRd9W5zrD9tJXWFBEvj4jPNqKdiLgpIjattJY57X3G9ofmrP8ZEfHNRqx/Gf6tpJdKGoiIE5f75Hq373xs/47tb9j+ue27V7KOFA7mfbL9Mds/sr3X9g9tb2lwecgUIQ400fSeZxv6DUl3R8RYgrbHJF0k6T8laDuVMUmnSvo1SW+S9HHbL0hbEqqAEEel2N5q+07bj9i+zvaGWY/N7PWWe6SftP1P5d7JTttPLR+7sXzK98su+D+cp53Ocu/mYdt3SXrFnMdnegdsP832/y73/B62feVC7Ux3odp+n+0HJF08t1u19Dzbt5Xd0RfbXlOu8822vzWnlihr2CbpDZLeW7b3v8rHZ7rnbR9i+zzb95f/zrN9SPnYdG3vsf2Q7T2237LIe7GhfA8eKd+TreX9b5V0oaSTyjo+cJDb9822v23772z/zPZdtl9Q3n9vWetM13tE3BwRl0iqt1fmC7YfKN+/G20/Y9ZjC36OysdfWu75/tz2JyR5kXZ6yvWN2r5N0vPmPD77fTq3rOvSst1bbP+m7bPL13uv7d+d9ZrPiYgfRsRUROyUdJOkk+p5/WhvhDgqw/bJkv5K0mmSniTpHklXLPKU10n6gKQ+SXdK+rAkRcSLysefXXbBXznPc7dKeqWk4yXVJL1mkXb+i6Qvl+0MSPrvS7TzRElHqthb3bbAOt8g6d9Jeqqk35T0F4u0r7K9CyRdJum/lu2dOs9ify7p+ZKeI+nZkk6cs+4nqtibO1rSWyV90nbfAk1eLmlI0gYV2+cjtk+JiE9LerukfynrOGee5y5n+0rSv5H0fyWtk/Q5Fe/78yQ9TdIZkj5h+7Al1rGQL0o6TtKvS9qtYhvONu/nyPZRkq5Wsf2OkvRjSZsXaeccFe/nU1W8t0sd8z9V0iVlu9+VdIOK7+SjJX1Q0j/M9yTbPSq2za1LrB+rACGOKnmDpIsiYndEPC7pbBV7exsXWP6acq9sUsUX83OW0dZpks6LiHsj4hEVfzws5JcqAnlDROyLiG8tsqwkTUk6JyIej4jxBZb5xKy2P6wiSBrhDZI+GBEPRcSwinB646zHf1k+/suIuF7So5J+5Xi97WNUHPd+X/mav6di7/uNc5ddwHK2ryT9JCIujoj9kq6UdExZ5+MR8WVJEyoCfdki4qKI2Ft+ps6V9GzbvzZrkYU+R78n6baIuCoifinpPEkPLNLUaZI+HBGPRMS9kv7bEqXdFBE3lO1+QVK/pI+WbV0haaPtI+Z53t9L+r6K0McqR4ijSjao2PuWJEXEo5JGVOyZzGf2F+pjkpazp7ZB0r2zbt+z0IKS3quiG/VmFyOM//0S6x6OiH1LLDO37Q0LLbhMB2zDedY9UobGtIW22wZJj0TE3jnrWui9mO/59W5fSXpw1u/jkhQRc+9b9p542a3/Uds/tv0LSXeXDx01a7GFPkcHvIYoZoua/ZrmOtjX/HD5R8z0bWnOa7b9N5KeKem0YPYqiBBHtdyvYo9XkmR7rYru1fua0NYeFXt705680IIR8UBEbI2IDZLeJulTXnxEej1frnPbvr/8fUzSodMP2H7iMtd9wDacs+7luF/SkbZ756yr3vei7u3bZK+X9CpJL1FxGGFjef+Cx7ZnOeA12LYOfE2LLq8Gv+Zy7MHLJf1uRPyiketGvghxVMnnJL3F9nPKwVgfkbQzIu5ewboelLTYOeWfl/Qu2wPlMeGzFlrQ9mttD5Q3R1UE6fQe01LtLOSPyraPlPR+FV3IUtFN+oxyG6xR0f0721LtXS7pL2z3l8d0/1LSpcstruwO/mdJf2V7je1nqTiGPvd48kLq3r7LZbuj3DZPKG56je3uBRbvlfS4ih6dQ1V8pur1Tyreiz9wcZbBu1SMKVjI5yWdbbuv/Ly8cxltLcr22Sr+IHlpRIw0ar3IHyGOqoiI+Jqk/6xiMNEeFQOETl/h+s6V9NlytPNp8zy+XcUxxe+rGOx0zSLrep6knbYflXSdpHdHxE/qbGchn1MxWO6u8t+HJCki/p+KQU1flfQjSXOPv39a0tPL9v7HPOv9kKRBFYPEbilf24fmWa4er1Ox53q/pGtVHOf/Sp3PXc72Xa4Xqehuvl7F3u64im05nx0qurXvk3SbpP9TbyMR8bCk10r6qIo/Ao6T9O1FnvKBsq2flPVcUm9bdfiIitf6o/KMgEdtv7+B60emzGEVpGZ7t4pBTPOFEgBgAeyJI6nynN3fVnGKDQBgGQhxJGP7r1V0O74vIpYayQsAmIPudAAAMsWeOAAAmSLEAQDIVBYzLB111FGxcePG1GUAANASu3btejgi+pdaLosQ37hxowYHB1OXAQBAS9iua7Av3ekAAGSKEAcAIFOEOAAAmSLEAQDIFCEOAECmCHEAADLVtBC3fZHth2z/YNZ9R9r+iu0flT/7mtU+AADtrpl74p+R9LI5950l6WsRcZykr5W3W25qKjS893HdN/qYhvc+rqkprh8PAMhP0y72EhE32t445+5XSXpx+ftnJX1T0vuaVcN8pqZCdzy4V1t3DGpodFwDfT3avqWmTet71dHhVpYCAMBBafUx8fURsUeSyp+/3uL2NTI2MRPgkjQ0Oq6tOwY1MjbR6lIAADgolR3YZnub7UHbg8PDww1b78Tk/pkAnzY0Oq6Jyf0NawMAgFZodYg/aPtJklT+fGihBSPigoioRUStv3/Ja8DXrburUwN9PQfcN9DXo+6uzoa1AQBAK7Q6xK+T9Kby9zdJ+p8tbl/r1nZr+5baTJBPHxNft7a71aUAAHBQmjawzfblKgaxHWV7SNI5kj4q6fO23yrpp5Je26z2F9LRYW1a36trz9ysicn96u7q1Lq13QxqAwBkp5mj01+3wEOnNKvNenV0WP29h6QuAwCAg1LZgW0AAGBxhDgAAJkixAEAyBQhDgBAppo2sA0A0N6mpkIjYxOc6ZMQIQ4AWDbmoagGutMBAMvGPBTVQIgDAJaNeSiqgRAHACwb81BUAyEOAFg25qGoBga2AQCWjXkoqoEQBwCsCPNQpEd3OgAAmSLEAQDIFCEOAECmCHEAADJFiAMAkClCHACATBHiAABkihAHACBThDgAAJkixAEAyBQhDgBApghxAAAyRYgDAJApQhwAgEwR4gAAZIoQBwAgU4Q4AACZIsQBAMgUIQ4AQKYIcQAAMkWIAwCQKUIcAIBMEeIAAGSKEAcAIFOEOAAAmSLEAQDIFCEOAECmulIXAABAKlNToZGxCU1M7ld3V6fWre1WR4dTl1U3QhwAsCpNTYXueHCvtu4Y1NDouAb6erR9S02b1vdmE+RJutNt/6ntW23/wPblttekqAMAsHqNjE3MBLgkDY2Oa+uOQY2MTSSurH4tD3HbR0t6l6RaRDxTUqek01tdBwBgdZuY3D8T4NOGRsc1Mbk/UUXLl2pgW5ekHttdkg6VdH+iOgAAq1R3V6cG+noOuG+gr0fdXZ2JKlq+lod4RNwn6WOSfippj6SfR8SX5y5ne5vtQduDw8PDrS4TANDm1q3t1vYttZkgnz4mvm5td+LK6ueIaG2Ddp+kqyX9oaSfSfqCpKsi4tKFnlOr1WJwcLBFFQIAVouDHZ3erNHttndFRG2p5VKMTn+JpJ9ExLAk2b5G0gskLRjiAAA0Q0eH1d97yIqeW4XR7SmOif9U0vNtH2rbkk6RdHuCOgAAWLEqjG5PcUx8p6SrJO2WdEtZwwWtrgMAgINRhdHtSS72EhHnSDonRdsAADTC9Oj22UHe6tHtXDsdAIAVqMLodi67CgDACnR0WJvW9+raMzcnu/Y6IQ4AwAodzOj2hrSfrGUAAHBQCHEAADJFiAMAkCmOiS8h9wnjgZXisw9UHyG+iCpcUg9Igc8+kAe60xdRhUvqASnw2QfyQIgvogqX1ANS4LMP5IEQX0Q7TBgPrASffSAPhPgiqnBJPSAFPvtAHhwRqWtYUq1Wi8HBwSRtM0IXqxWffSAd27siorbUcoxOX0LqS+oBqfDZB6qP7nQAADJFiAMAkClCHACATBHiAABkihAHACBThDgAAJkixAEAyBQhDgBApghxAAAyRYgDAJApQhwAgEwR4gAAZIoQBwAgU4Q4AACZIsQBAMgUIQ4AQKYIcQAAMkWIAwCQKUIcAIBMEeIAAGSKEAcAIFOEOAAAmSLEAQDIFCEOAECmCHEAADJFiAMAkKkkIW77CNtX2f6h7dttn5SiDgAActaVqN2PS/pSRLzGdrekQxPVAQBAtloe4rYPl/QiSW+WpIiYkDTR6joAAMhdiu70YyUNS7rY9ndtX2h7bYI6AADIWooQ75L0XEnnR8TxksYknTV3IdvbbA/aHhweHm51jQAAVF6KEB+SNBQRO8vbV6kI9QNExAURUYuIWn9/f0sLBAAgBy0P8Yh4QNK9tjeVd50i6bZW1wEAQO5SjU5/p6TLypHpd0l6S6I6AADIVpIQj4jvSaqlaBsAgHbBFdsAAMgUIQ4AQKYIcQAAMkWIAwCQKUIcAIBMEeIAAGQq1XniAABkZ2oqNDI2oYnJ/eru6tS6td3q6HCyeghxAADqMDUVuuPBvdq6Y1BDo+Ma6OvR9i01bVrfmyzI6U4HAKAOI2MTMwEuSUOj49q6Y1AjY+lm0ybEAQCow8Tk/pkAnzY0Oq6Jyf2JKqojxG0fbvup89z/rOaUBABA9XR3dWqgr+eA+wb6etTd1ZmooiVC3PZpkn4o6Wrbt9p+3qyHP9PMwgAAqJJ1a7u1fUttJsinj4mvW9udrKalBra9X9IJEbHH9omSLrH9/oi4RlK64XgAALRYR4e1aX2vrj1zczaj0zsjYo8kRcTNtn9H0j/aHpAUTa8OAIAK6eiw+nsPSV3GjKWOie+dfTy8DPQXS3qVpGc0sS4AALCEpfbE36E53eYRsdf2yySd1rSqAADAkhYN8Yj4/uzbtg+f9ZwvNqsoAACwtLqu2Gb7bZI+KGlc/3osPCQd26S6AADAEuq97Op/lPSMiHi4mcUAAID61XvFth9LeqyZhQAAgOWpd0/8bEn/bHunpMen74yIdzWlKgAAsKR6Q/wfJH1d0i2SpppXDgAAqFe9IT4ZEX/W1EoAAMCy1HtM/Bu2t9l+ku0jp/81tTIAALCoevfEX1/+PHvWfZxiBgBAQnWFeEQ8pdmFAACA5Vk0xG2fHBFft/0H8z1ezmYGAAASWGpP/EUqRqWfqqL73HN+EuIAgFVnaio0MjaRfErSpUJ8r+0/k/QD/Wt4S0xDCgBYpaamQnc8uFdbdwxqaHRcA3092r6lpk3re1se5EuNTj9MUq+kE1TMaPYkSRskvV3S05tbGgAA1TMyNjET4JI0NDqurTsGNTI20fJalprF7AOSZPvLkp4bEXvL2+dK+kLTqwMAoGImJvfPBPi0odFxTUzub3kt9Z4n/mRJs//EmJC0seHVAABQcd1dnRro6zngvoG+HnV3dba8lnpD/BJJN9s+1/Y5knZK+mzzygIAoJrWre3W9i21mSCfPia+bm13y2txRH1j1Gw/V9ILy5s3RsR3m1bVHLVaLQYHB1vVHAAAi2r26HTbuyKittRy9V6xTRGxW9Lug6oKAIA20NFh9fcekrqM+kMcALCwqpw3jNWFEEcyfOmhXVTpvGGsLvUObAMaavpL79Wf+rY2//U39OpPfVt3PLhXU1NcRwj5qdJ5w1hdCHEkwZce2kmVzhvG6kKIIwm+9NBOqnTeMFYXQhxJ8KWHdlKl84axutR9nnjDG7Y7JQ1Kui8iXrnYspwn3n4YCIR2w0BNNFLDzxNvgndLul3S4QlrQCIdHdam9b269szNfOmhLVTlvGGsLkm6020PSHqFpAtTtI9qmP7SO7rvUPX3HkKAA8AypTomfp6k90qaWmgB29tsD9oeHB4ebl1lAABkouUhbvuVkh6KiF2LLRcRF0RELSJq/f39LaoOAIB8pNgT3yzp923fLekKSSfbvjRBHQAAZK3lIR4RZ0fEQERslHS6pK9HxBmtrgMAgNxxnjgAAJlKOgFKRHxT0jdT1gAAQK7YEwcAIFOEOAAAmWI+cQCoCC7diuUixAGgAphPACtBdzoAVMDI2MRMgEvF1LxbdwxqZGwicWWoMkIcACpgYnL/TIBPGxod18Tk/kQVIQeEOABUQHdX58x85NMG+nrU3dWZqCLkgBAHgApYt7Zb27fUZoJ8+pj4urXdiStDlTGwDQAqoKPD2rS+V9eeuZnR6agbIQ4AFdHRYfX3HpK6DGSE7nQAADJFiAMAkClCHACATBHiAABkihAHACBTjE4H0DBM4AG0FiEOoCGYwANoPbrTATQEE3gArUeIA2gIJvAAWo8QB9AQTOABtB4hDqAhmMADaD0GtgFoCCbwAFqPEAfQMEzgAbQW3ekAAGSKPXEAB4ULvADpEOIAVowLvABp0Z0OYMW4wAuQFiEOYMW4wAuQFiEOYMW4wAuQFiEOYMW4wAuQFgPbAKwYF3gB0iLEARwULvACpEOIA0AinGOPg0WIA0ACnGOPRmBgGwAkwDn2aAT2xAEgAc6xb712PHxBiANAAtPn2M8Ocs6xb552PXxBdzoAJMA59q3Vrocv2BMHgAQ4x7612vXwBSEOAIlwjn3rtOvhi5Z3p9s+xvY3bN9u+1bb7251DQCA1aVdD1+k2BOflPSeiNhtu1fSLttfiYjbEtQCAFgF2vXwRctDPCL2SNpT/r7X9u2SjpZEiAMAmqYdD18kPSZue6Ok4yXtTFkHgMZpx3NxgapKFuK2D5N0taQ/iYhfzPP4NknbJOnJT35yi6sDsBLtei4uUFVJzhO3/QQVAX5ZRFwz3zIRcUFE1CKi1t/f39oCAaxIu56LC1RVitHplvRpSbdHxN+2un0AzdOu5+ICVZViT3yzpDdKOtn298p/v5egDgANNn0u7mztcC4uUFUtD/GI+FZEOCKeFRHPKf9d3+o6ADReu56LC1QVV2wD0DDtei4uUFWEOICGasdzcYGqYhYzAAAyRYgDAJApQhwAgEwR4gAAZIoQBwAgU4Q4AACZIsQBAMgUIQ4AQKYIcQAAMkWIAwCQKUIcAIBMEeIAAGSKCVDQtqamQiNjE8ymBaBtEeJoS1NToTse3KutOwY1NDo+M6/1pvW9BDmAtkF3OtrSyNjETIBL0tDouLbuGNTI2ETiygCgcQhxtKWJyf0zAT5taHRcE5P7E1UEAI1HiKMtdXd1aqCv54D7Bvp61N3VmagiAGg8Qhxtad3abm3fUpsJ8ulj4uvWdieuDAAah4FtaEsdHdam9b269szNyUanV2F0fBVqANA8hDjaVkeH1d97SJK2qzA6vgo1AGguutOBJqjC6Pgq1ACguQhxoAmqMDq+CjUAaC5CHGiCKoyOr0INAJqLEAeaoAqj46tQA4DmckSkrmFJtVotBgcHU5cBLEsVRoZXoQYAy2d7V0TUllqO0elAk6QcHV+lGgA0DyGOBbEXBwDVRohjXpxjDADVx8A2zItzjAGg+ghxzItzjAGg+ghxzItzjAGg+ghxzItzjAGg+hjYhnlVYRYwAMDiCHEsiHOMAaDa6E4HACBThDgAAJkixAEAyBQhDgBApghxAAAylSTEbb/M9h2277R9Vqva3bdvUveNPqZ7RsZ03+hjGtu3T/v2TbaqeQAAGqrlp5jZ7pT0SUkvlTQk6Tu2r4uI25rZ7r59k/rRyJjecemumQk9zj/jBB27Ttq3T1qzhrPtAAB5SbEnfqKkOyPiroiYkHSFpFc1u9GR8YmZAJeK64C/49Jd+tn4lEbGmdQDAJCfFCF+tKR7Z90eKu87gO1ttgdtDw4PDx90o5NTMe+EHpNTocmpOOj1AwDQailCfL7rdv5KikbEBRFRi4haf3//QTfa1eF5J/To6rC6uJQoACBDKUJ8SNIxs24PSLq/2Y2u6+nW+WeccMCEHuefcYKO6OnQuh4m9QAA5CfFaK7vSDrO9lMk3SfpdEmvb3aja9Z06bh1a3XltudrcirU1WEd0dOhTnUxqA0AkKWWp1dETNr+Y0k3SOqUdFFE3NqKttes6dLRBDYAoE0kSbSIuF7S9SnaBgCgXXDFNgAAMkWIAwCQKUIcAIBMEeIAAGSKEAcAIFOEOAAAmSLEAQDIlCOqP/mH7WFJ9zRwlUdJeriB68OvYhs3H9u4+djGzcc2nt9vRMSSE4dkEeKNZnswImqp62hnbOPmYxs3H9u4+djGB4fudAAAMkWIAwCQqdUa4hekLmAVYBs3H9u4+djGzcc2Pgir8pg4AADtYLXuiQMAkL1VF+K2X2b7Dtt32j4rdT3txvYxtr9h+3bbt9p+d+qa2pHtTtvftf2PqWtpV7aPsH2V7R+Wn+eTUtfUTmz/afkd8QPbl9tek7qmHK2qELfdKemTkl4u6emSXmf76WmrajuTkt4TEb8t6fmS/oht3BTvlnR76iLa3MclfSkifkvSs8X2bhjbR0t6l6RaRDxTUqek09NWladVFeKSTpR0Z0TcFRETkq6Q9KrENbWViNgTEbvL3/eq+OI7Om1V7cX2gKRXSLowdS3tyvbhkl4k6dOSFBETEfGztFW1nS5JPba7JB0q6f7E9WRptYX40ZLunXV7SARM09jeKOl4STvTVtJ2zpP0XklTqQtpY8dKGpZ0cXnY4kLba1MX1S4i4j5JH5P0U0l7JP08Ir6ctqo8rbYQ9zz3MTy/CWwfJulqSX8SEb9IXU+7sP1KSQ9FxK7UtbS5LknPlXR+RBwvaUwSY2gaxHafil7Qp0jaIGmt7TPSVpWn1RbiQ5KOmXV7QHThNJztJ6gI8Msi4prU9bSZzZJ+3/bdKg4HnWz70rQltaUhSUMRMd2LdJWKUEdjvETSTyJiOCJ+KekaSS9IXFOWVluIf0fScbafYrtbxUCK6xLX1FZsW8VxxNsj4m9T19NuIuLsiBiIiI0qPr9fjwj2YBosIh6QdK/tTeVdp0i6LWFJ7eankp5v+9DyO+MUMXBwRbpSF9BKETFp+48l3aBiNORFEXFr4rLazWZJb5R0i+3vlfe9PyKuT1gTsBLvlHRZ+Qf/XZLekriethERO21fJWm3ijNaviuu3LYiXLENAIBMrbbudAAA2gYhDgBApghxAAAyRYgDAJApQhwAgEytqlPMABzI9rmSHpV0uKQbI+Kry3jul1RMcvOtiHhlcyoEsBhCHIAi4i9X8LS/UTFxxdsaXA6AOtGdDqwytv/c9h22vyppU3nfZ2y/pvz9btsfsf0vtgdtP9f2DbZ/bPvt0+uJiK9J2pvmVQCQ2BMHVhXbJ6i4XOvxKv7/75Y032Qq90bESbb/TtJnVFyJb42kWyX9fWuqBbAUQhxYXV4o6dqIeEySbC80d8D0/bdIOqycG36v7X22j2BubaAa6E4HVp96rrX8ePlzatbv07f54x+oCEIcWF1ulPRq2z22eyWdmrogACvHX9TAKhIRu21fKel7ku6RdNNK12X7Jkm/Jekw20OS3hoRNzSmUgD1YBYzAAAyRXc6AACZIsQBAMgUIQ4AQKYIcQAAMkWIAwCQKUIcAIBMEeIAAGSKEAcAIFP/H5NptK/C6WpDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualising the data to check for any outliers\n",
    "\n",
    "# Plotting a scatter plot\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.scatterplot(x=X_train['dim2'], y=X_train['dim1'])\n",
    "\n",
    "# Setting the title and labels\n",
    "plt.title(\"Joint distribution of dim1 and dim2\")\n",
    "plt.xlabel('dim1')\n",
    "plt.ylabel('dim2')\n",
    "\n",
    "# displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the dataset to improve data sparsity \n",
    "\n",
    "# Cleaning the copied training dataset by setting threshold values\n",
    "X_tr_cleaned = X_train.copy() \n",
    "X_tr_cleaned[X_tr_cleaned > 1] = None \n",
    "X_tr_cleaned = X_tr_cleaned.dropna(thresh=500) \n",
    "\n",
    "# Calculating the difference in indexes after X was cleaned\n",
    "x_index_set = set(X_tr_cleaned.index.values)\n",
    "y_index_set = set(y_train.index.values)\n",
    "\n",
    "indexes_diff = list(y_index_set - x_index_set)\n",
    "\n",
    "# cleaning the training true labels and scaling X \n",
    "y_tr_cleaned = np.array((y_train.drop(index=indexes_diff)).reset_index().drop(columns='index')).reshape(-1,) #convert to a numpy array\n",
    "X_tr_scaled = pd.DataFrame((preprocessing.scale(X_tr_cleaned)), columns = X_train.columns) \n",
    "\n",
    "# Copying values to default variables\n",
    "X_train = X_tr_scaled.copy()\n",
    "y_train = y_tr_cleaned.copy()\n",
    "\n",
    "# Printing shapes\n",
    "print(\"X_train shape {0}, y_train shape {1}\".format(X_train.shape, y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualising the data to check for data distribution - post-clean\n",
    "\n",
    "# Plotting a scatter plot\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.scatterplot(x=X_train['dim2'], y=X_train['dim1'])\n",
    "\n",
    "# Setting the title and labels\n",
    "plt.title(\"Joint distribution of dim1 and dim2\")\n",
    "plt.xlabel('dim1')\n",
    "plt.ylabel('dim2')\n",
    "\n",
    "# displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the KFold validation object to improve the training accuracy\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric Output Functions\n",
    "\n",
    "# Calulating and printing the accuracy of the classifier on the training and validation sets\n",
    "def accuracy_out(X_train, X_valid, y_train, y_valid, pred_train, pred_valid):\n",
    "    \n",
    "    print(\"Classification Accuracy:\")\n",
    "    accuracy_train = accuracy_score(y_true=y_train, y_pred=pred_train)\n",
    "    print(\"Training    - Classification Accuracy - {0:.3f} (to 3 decimal places).\".format(accuracy_train))\n",
    "    accuracy_valid = accuracy_score(y_true=y_valid, y_pred=pred_valid)\n",
    "    print(\"Validation  - Classification Accuracy - {0:.3f} (to 3 decimal places).\".format(accuracy_valid))\n",
    "\n",
    "\n",
    "# Calculating and printing the logarithmic loss of the classifier on the training and validation sets\n",
    "def log_loss_out(X_train, X_valid, y_train, y_valid, pred_train, pred_valid):\n",
    "    \n",
    "    print(\"\\nLogarithmic Loss:\")\n",
    "    logloss_train = log_loss(y_true=y_train, y_pred=pred_train)\n",
    "    print(\"Training    - Logarithmic Loss - {0:.3f} (to 3 decimal places).\".format(logloss_train))\n",
    "    logloss_valid = log_loss(y_true=y_valid, y_pred=pred_valid)\n",
    "    print(\"Validation  - Logarithmic Loss - {0:.3f} (to 3 decimal places).\".format(logloss_valid))\n",
    "    \n",
    "\n",
    "# Calculating and printing the r^2 score of the classifier on the training and testing sets\n",
    "def r2_score_out(X_train, X_valid, y_train, y_valid, pred_train, pred_valid):\n",
    "    \n",
    "    print(\"R^2 Score:\")\n",
    "    r_2_train = r2_score(y_true=y_train, y_pred=pred_train)\n",
    "    print(\"Training - R^2 Score - {0:.3f} (to 3 decimal places).\".format(r_2_train))\n",
    "    r_2_valid = r2_score(y_true=y_valid, y_pred=pred_valid)\n",
    "    print(\"Testing  - R^2 Score - {0:.3f} (to 3 decimal places).\".format(r_2_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline\n",
      "--------\n",
      "\n",
      "Classification Accuracy:\n",
      "Training    - Classification Accuracy - 0.551 (to 3 decimal places).\n",
      "Validation  - Classification Accuracy - 0.527 (to 3 decimal places).\n",
      "\n",
      "Logarithmic Loss:\n",
      "Training    - Logarithmic Loss - 15.512 (to 3 decimal places).\n",
      "Validation  - Logarithmic Loss - 16.354 (to 3 decimal places).\n"
     ]
    }
   ],
   "source": [
    "# Baseline Classifier - Coin Flip Model\n",
    "\n",
    "# Creating the constant value Dummy model and training it to act as a baseline\n",
    "dm_clf = DummyClassifier(strategy='constant', constant=0, random_state=0)\n",
    "dm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the values of the training and testing data\n",
    "pred_train = dm_clf.predict(X_train)\n",
    "pred_valid = dm_clf.predict(X_valid)\n",
    "\n",
    "## Success Metrics\n",
    "\n",
    "print(\"\\nBaseline\")\n",
    "print(\"--------\\n\")\n",
    "\n",
    "accuracy_out(X_train, X_valid, y_train, y_valid, pred_train, pred_valid)\n",
    "log_loss_out(X_train, X_valid, y_train, y_valid, pred_train, pred_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes\n",
    "\n",
    "# # Creating the model and training it\n",
    "# nb_clf = MultinomialNB()\n",
    "# nb_clf.fit(X_train, y_train)\n",
    "\n",
    "# # Predicting the values of the training and testing data\n",
    "# pred_train = nb_clf.predict(X_train)\n",
    "# pred_valid = nb_clf.predict(X_valid)\n",
    "\n",
    "# ## Success Metrics\n",
    "\n",
    "# print(\"\\nMultinomial Naive Bayes\")\n",
    "# print(\"-----------------------\\n\")\n",
    "\n",
    "# accuracy_out(X_train, X_valid, y_train, y_valid, pred_train, pred_valid)\n",
    "# log_loss_out(X_train, X_valid, y_train, y_valid, pred_train, pred_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression\n",
      "-----------------\n",
      "\n",
      "R^2 Score:\n",
      "Training - R^2 Score - 0.440 (to 3 decimal places).\n",
      "Testing  - R^2 Score - -193.671 (to 3 decimal places).\n",
      "\n",
      "Logarithmic Loss:\n",
      "Training    - Logarithmic Loss - 0.488 (to 3 decimal places).\n",
      "Validation  - Logarithmic Loss - 13.813 (to 3 decimal places).\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "\n",
    "# Creating the model and training it\n",
    "lir_clf = LinearRegression()\n",
    "lir_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the values of the training and testing data\n",
    "pred_train = lir_clf.predict(X_train)\n",
    "pred_valid = lir_clf.predict(X_valid)\n",
    "\n",
    "## Success Metrics\n",
    "\n",
    "print(\"\\nLinear Regression\")\n",
    "print(\"-----------------\\n\")\n",
    "\n",
    "r2_score_out(X_train, X_valid, y_train, y_valid, pred_train, pred_valid)\n",
    "log_loss_out(X_train, X_valid, y_train, y_valid, pred_train, pred_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression\n",
      "-------------------\n",
      "\n",
      "Classification Accuracy:\n",
      "Training    - Classification Accuracy - 0.811 (to 3 decimal places).\n",
      "Validation  - Classification Accuracy - 0.643 (to 3 decimal places).\n",
      "\n",
      "Logarithmic Loss:\n",
      "Training    - Logarithmic Loss - 6.535 (to 3 decimal places).\n",
      "Validation  - Logarithmic Loss - 12.320 (to 3 decimal places).\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# Creating the model and training it\n",
    "lor_clf = LogisticRegression()\n",
    "lor_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the values of the training and testing data\n",
    "pred_train = lor_clf.predict(X_train)\n",
    "pred_valid = lor_clf.predict(X_valid)\n",
    "\n",
    "## Success Metrics\n",
    "\n",
    "print(\"\\nLogistic Regression\")\n",
    "print(\"-------------------\\n\")\n",
    "\n",
    "accuracy_out(X_train, X_valid, y_train, y_valid, pred_train, pred_valid)\n",
    "log_loss_out(X_train, X_valid, y_train, y_valid, pred_train, pred_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating the Best kNN classifier\n",
      "-----------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The most accurate classifier was 40-NN with logarithmic loss 10.706 on the test set (to 3 decimal places).\n",
      "\n",
      "\n",
      "KNN Classifier\n",
      "--------------\n",
      "\n",
      "Classification Accuracy:\n",
      "Training    - Classification Accuracy - 0.697 (to 3 decimal places).\n",
      "Validation  - Classification Accuracy - 0.690 (to 3 decimal places).\n",
      "\n",
      "Logarithmic Loss:\n",
      "Training    - Logarithmic Loss - 10.479 (to 3 decimal places).\n",
      "Validation  - Logarithmic Loss - 10.706 (to 3 decimal places).\n"
     ]
    }
   ],
   "source": [
    "# KNN model\n",
    "\n",
    "## Training the classifier multiple times, each time setting the KNN option to a different value:\n",
    "\n",
    "print(\"\\nCalculating the Best kNN classifier\")\n",
    "print(\"-----------------------------------\\n\")\n",
    "\n",
    "# List of the k values\n",
    "k_value_list = list(range(1,50)) + [100, 200, 500, 1000, 1500, 2000]\n",
    "clas_ll = []\n",
    "\n",
    "# Storing the best perfoming knn classifier values \n",
    "best_k_value = k_value_list[0]\n",
    "best_knn_classifier = KNeighboursClassifier(n_neighbors=best_k_value)\n",
    "min_logloss = 100\n",
    "\n",
    "for no_classifier, k_value in enumerate(k_value_list):\n",
    "    \n",
    "    # Training the KNeighboursClassifier\n",
    "    knc = KNeighboursClassifier(n_neighbors=k_value)\n",
    "    knc.fit(X=X_train, y=y_train)\n",
    "\n",
    "    # Predicting the output of the training set\n",
    "    y_pred = knc.predict(X_valid)\n",
    "\n",
    "    # Calculating and printing the logarithmic loss\n",
    "    logloss = log_loss(y_true=y_valid, y_pred=y_pred)\n",
    "    clas_ll.append(logloss) # storing the logarithmic loss values\n",
    "          \n",
    "    # Storing the best k-value\n",
    "    if logloss < min_logloss:\n",
    "        min_logloss = logloss\n",
    "        best_k_value = k_value\n",
    "        best_knn_classifier = knc\n",
    "\n",
    "## Plotting the results (k-value on the x-axis and classification accuracy on the y-axis), marking the axes too:\n",
    "\n",
    "# Creating the bar graph\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(k_value_list, clas_ll)\n",
    "plt.plot(k_value_list,clas_ll, )\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"K-nearest neighbours classification\")\n",
    "plt.xlabel(\"K Values\")\n",
    "plt.ylabel(\"Logarithmic Loss\")\n",
    "\n",
    "# Plotting the graph\n",
    "plt.show()\n",
    "\n",
    "# Providing output about which was the best classifier\n",
    "print(\"\\nThe most accurate classifier was {0}-NN with logarithmic loss {1:.3f} on the test set (to 3 decimal places).\\n\"\n",
    "      .format(best_k_value, min_logloss))\n",
    "\n",
    "\n",
    "## Using the best chosen knn classifier\n",
    "\n",
    "knn_clf = best_knn_classifier\n",
    "\n",
    "# Predicting the values of the training and testing data\n",
    "pred_train = knn_clf.predict(X_train)\n",
    "pred_valid = knn_clf.predict(X_valid)\n",
    "\n",
    "## Success Metrics\n",
    "\n",
    "print(\"\\nKNN Classifier\")\n",
    "print(\"--------------\\n\")\n",
    "\n",
    "accuracy_out(X_train, X_valid, y_train, y_valid, pred_train, pred_valid)\n",
    "log_loss_out(X_train, X_valid, y_train, y_valid, pred_train, pred_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM - Linear Classifier\n",
      "--------------------\n",
      "\n",
      "Classification Accuracy:\n",
      "Training    - Classification Accuracy - 0.842 (to 3 decimal places).\n",
      "Validation  - Classification Accuracy - 0.630 (to 3 decimal places).\n",
      "\n",
      "Logarithmic Loss:\n",
      "Training    - Logarithmic Loss - 5.446 (to 3 decimal places).\n",
      "Validation  - Logarithmic Loss - 12.785 (to 3 decimal places).\n",
      "\n",
      "SVM - RBF Classifier\n",
      "--------------------\n",
      "\n",
      "Classification Accuracy:\n",
      "Training    - Classification Accuracy - 0.911 (to 3 decimal places).\n",
      "Validation  - Classification Accuracy - 0.714 (to 3 decimal places).\n",
      "\n",
      "Logarithmic Loss:\n",
      "Training    - Logarithmic Loss - 3.086 (to 3 decimal places).\n",
      "Validation  - Logarithmic Loss - 9.868 (to 3 decimal places).\n",
      "\n",
      "SVM - Poly Classifier\n",
      "--------------------\n",
      "\n",
      "Classification Accuracy:\n",
      "Training    - Classification Accuracy - 0.940 (to 3 decimal places).\n",
      "Validation  - Classification Accuracy - 0.684 (to 3 decimal places).\n",
      "\n",
      "Logarithmic Loss:\n",
      "Training    - Logarithmic Loss - 2.079 (to 3 decimal places).\n",
      "Validation  - Logarithmic Loss - 10.923 (to 3 decimal places).\n"
     ]
    }
   ],
   "source": [
    "#  SVMs\n",
    "\n",
    "# Training an SVM with a Linear Kernel\n",
    "svm_linear = SVC(kernel='linear')\n",
    "svm_linear.fit(X=X_train, y=y_train)\n",
    "\n",
    "# Predicting the values of the training and testing data\n",
    "pred_train = svm_linear.predict(X_train)\n",
    "pred_valid = svm_linear.predict(X_valid)\n",
    "\n",
    "## Success Metrics\n",
    "\n",
    "print(\"\\nSVM - Linear Classifier\")\n",
    "print(\"--------------------\\n\")\n",
    "\n",
    "accuracy_out(X_train, X_valid, y_train, y_valid, pred_train, pred_valid)\n",
    "log_loss_out(X_train, X_valid, y_train, y_valid, pred_train, pred_valid)\n",
    "\n",
    "# Training an SVM with a RBF Kernel\n",
    "svm_rbf = SVC(kernel='rbf')\n",
    "svm_rbf.fit(X=X_train, y=y_train)\n",
    "\n",
    "# Predicting the values of the training and testing data\n",
    "pred_train = svm_rbf.predict(X_train)\n",
    "pred_valid = svm_rbf.predict(X_valid)\n",
    "\n",
    "## Success Metrics\n",
    "\n",
    "print(\"\\nSVM - RBF Classifier\")\n",
    "print(\"--------------------\\n\")\n",
    "\n",
    "accuracy_out(X_train, X_valid, y_train, y_valid, pred_train, pred_valid)\n",
    "log_loss_out(X_train, X_valid, y_train, y_valid, pred_train, pred_valid)\n",
    "\n",
    "# Training an SVM with a 2nd-order degree polynomial Kernel\n",
    "svm_poly = SVC(kernel='poly', degree=2)\n",
    "svm_poly.fit(X=X_train, y=y_train)\n",
    "\n",
    "# Predicting the values of the training and testing data\n",
    "pred_train = svm_poly.predict(X_train)\n",
    "pred_valid = svm_poly.predict(X_valid)\n",
    "\n",
    "## Success Metrics\n",
    "\n",
    "print(\"\\nSVM - Poly Classifier\")\n",
    "print(\"--------------------\\n\")\n",
    "\n",
    "accuracy_out(X_train, X_valid, y_train, y_valid, pred_train, pred_valid)\n",
    "log_loss_out(X_train, X_valid, y_train, y_valid, pred_train, pred_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best value of C is: 1\n"
     ]
    }
   ],
   "source": [
    "# Optimising the SVM rbf function given that it has the best results\n",
    "# of the SVMs\n",
    "\n",
    "# initialising the folds and possible values of c\n",
    "no_folds = 5\n",
    "c_vals = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4, 1e5]\n",
    "\n",
    "# initialising the folds and array\n",
    "kf = KFold(n_splits=no_folds, shuffle=True, random_state=0)\n",
    "arr = np.zeros((5,11))\n",
    "\n",
    "# lopping over each possible value of c\n",
    "for no, c_val in enumerate(c_vals):\n",
    "    \n",
    "    # calculating the cross val score for each training classifier with value of c\n",
    "    svm_rbf = SVC(kernel='rbf', C=c_val)\n",
    "    svm_rbf.fit(X=X_train, y=y_train)\n",
    "    score = cross_val_score(svm_rbf, X_train, y_train, cv=kf)\n",
    "    for fold in range(no_folds):    # For each fold\n",
    "        arr[fold, no] = score[fold]\n",
    "\n",
    "# printing the best value of c\n",
    "best_c_val = c_vals[np.argmax(np.mean(arr,0))]\n",
    "print(\"The best value of C is: {0}\".format(best_c_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy:\n",
      "Training    - Classification Accuracy - 0.911 (to 3 decimal places).\n",
      "Validation  - Classification Accuracy - 0.714 (to 3 decimal places).\n",
      "\n",
      "Logarithmic Loss:\n",
      "Training    - Logarithmic Loss - 3.086 (to 3 decimal places).\n",
      "Validation  - Logarithmic Loss - 9.868 (to 3 decimal places).\n"
     ]
    }
   ],
   "source": [
    "# Taking the best value of c and incorporating it into our model\n",
    "\n",
    "svm_rbf.probability = True\n",
    "svm_rbf.set_params(C=best_c_val)\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the values of the training and testing data\n",
    "pred_train = svm_rbf.predict(X_train)\n",
    "pred_valid = svm_rbf.predict(X_valid)\n",
    "\n",
    "# Success metrics\n",
    "accuracy_out(X_train, X_valid, y_train, y_valid, pred_train, pred_valid)\n",
    "log_loss_out(X_train, X_valid, y_train, y_valid, pred_train, pred_valid)\n",
    "\n",
    "# Computing the test set predicted probabilities\n",
    "pred_probabilities = svm_rbf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ANSWER_TEXT#\n",
    "\n",
    "***My answer:***\n",
    "\n",
    "My initial strategy for solving this mini challange was to try each of the classifiers I have learned in class one by one. However, I quickly found this was a fools errand and revised my approach to visualise and clean the data. Doing this allowed me to get better results almost immediately. Continuing to work on the other courseworks, I was able to improve my optimisation of classifiers and found SVMs to be useful for this task. \n",
    "\n",
    "In my solution, I have optimised the rbf SVM with the best value for C. I choose this particular SVM as it had the best values of log_loss and classification in comparison with the alternative SVMs. However, the C optimisation was not as effective as I hoped, and the C value that was returned was 1. This meant that the results after the C search were the same as before it. I feel with further optimisation this could certainly be improved.\n",
    "\n",
    "I have enjoyed this mini challenge, and think I have learned a lot from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20608068, 0.79391932\n",
      "0.20678762, 0.79321238\n",
      "0.20516361, 0.79483639\n",
      "0.20503277, 0.79496723\n",
      "0.20583845, 0.79416155\n",
      "0.20393996, 0.79606004\n",
      "0.20461600, 0.79538400\n",
      "0.20395726, 0.79604274\n",
      "0.20736828, 0.79263172\n",
      "0.20566317, 0.79433683\n",
      "0.20485384, 0.79514616\n",
      "0.20651068, 0.79348932\n",
      "0.20593622, 0.79406378\n",
      "0.20550802, 0.79449198\n",
      "0.20368622, 0.79631378\n",
      "0.20498858, 0.79501142\n",
      "0.20361369, 0.79638631\n",
      "0.20610371, 0.79389629\n",
      "0.20614425, 0.79385575\n",
      "0.20566398, 0.79433602\n",
      "0.20758395, 0.79241605\n",
      "0.20545275, 0.79454725\n",
      "0.20583771, 0.79416229\n",
      "0.20411817, 0.79588183\n",
      "0.20587676, 0.79412324\n",
      "0.20377439, 0.79622561\n",
      "0.20416992, 0.79583008\n",
      "0.20658392, 0.79341608\n",
      "0.20510272, 0.79489728\n",
      "0.20451240, 0.79548760\n",
      "0.20575912, 0.79424088\n",
      "0.20485352, 0.79514648\n",
      "0.20695729, 0.79304271\n",
      "0.20501023, 0.79498977\n",
      "0.20591104, 0.79408896\n",
      "0.20518604, 0.79481396\n",
      "0.20433370, 0.79566630\n",
      "0.20649021, 0.79350979\n",
      "0.20655141, 0.79344859\n",
      "0.20533406, 0.79466594\n",
      "0.20550029, 0.79449971\n",
      "0.20500983, 0.79499017\n",
      "0.20509907, 0.79490093\n",
      "0.20625968, 0.79374032\n",
      "0.20340845, 0.79659155\n",
      "0.20476316, 0.79523684\n",
      "0.20439759, 0.79560241\n",
      "0.20788827, 0.79211173\n",
      "0.20437516, 0.79562484\n",
      "0.20687437, 0.79312563\n",
      "0.20662355, 0.79337645\n",
      "0.20527268, 0.79472732\n",
      "0.20473422, 0.79526578\n",
      "0.20450179, 0.79549821\n",
      "0.20461500, 0.79538500\n",
      "0.20441574, 0.79558426\n",
      "0.20519177, 0.79480823\n",
      "0.20548375, 0.79451625\n",
      "0.20509868, 0.79490132\n",
      "0.20404145, 0.79595855\n",
      "0.20679019, 0.79320981\n",
      "0.20647159, 0.79352841\n",
      "0.20641614, 0.79358386\n",
      "0.20466027, 0.79533973\n",
      "0.20550316, 0.79449684\n",
      "0.20457717, 0.79542283\n",
      "0.20540071, 0.79459929\n",
      "0.20687990, 0.79312010\n",
      "0.20417958, 0.79582042\n",
      "0.20609734, 0.79390266\n",
      "0.20532310, 0.79467690\n",
      "0.20568549, 0.79431451\n",
      "0.20267406, 0.79732594\n",
      "0.20604877, 0.79395123\n",
      "0.20560541, 0.79439459\n",
      "0.20380399, 0.79619601\n",
      "0.20515169, 0.79484831\n",
      "0.20575923, 0.79424077\n",
      "0.20494728, 0.79505272\n",
      "0.20494755, 0.79505245\n",
      "0.20452168, 0.79547832\n",
      "0.20589791, 0.79410209\n",
      "0.20505461, 0.79494539\n",
      "0.20439263, 0.79560737\n",
      "0.20617718, 0.79382282\n",
      "0.20363295, 0.79636705\n",
      "0.20459004, 0.79540996\n",
      "0.20544369, 0.79455631\n",
      "0.20596982, 0.79403018\n",
      "0.20502328, 0.79497672\n",
      "0.20562590, 0.79437410\n",
      "0.20438687, 0.79561313\n",
      "0.20434239, 0.79565761\n",
      "0.20505131, 0.79494869\n",
      "0.20446851, 0.79553149\n",
      "0.20493912, 0.79506088\n",
      "0.20639518, 0.79360482\n",
      "0.20549410, 0.79450590\n",
      "0.20471972, 0.79528028\n",
      "0.20424638, 0.79575362\n",
      "0.20645269, 0.79354731\n",
      "0.20492370, 0.79507630\n",
      "0.20663476, 0.79336524\n",
      "0.20536909, 0.79463091\n",
      "0.20434273, 0.79565727\n",
      "0.20621586, 0.79378414\n",
      "0.20455620, 0.79544380\n",
      "0.20439736, 0.79560264\n",
      "0.20353859, 0.79646141\n",
      "0.20670758, 0.79329242\n",
      "0.20529472, 0.79470528\n",
      "0.20377056, 0.79622944\n",
      "0.20528975, 0.79471025\n",
      "0.20609363, 0.79390637\n",
      "0.20496291, 0.79503709\n",
      "0.20715254, 0.79284746\n",
      "0.20482797, 0.79517203\n",
      "0.20518598, 0.79481402\n",
      "0.20481533, 0.79518467\n",
      "0.20423899, 0.79576101\n",
      "0.20651721, 0.79348279\n",
      "0.20544727, 0.79455273\n",
      "0.20825019, 0.79174981\n",
      "0.20495735, 0.79504265\n",
      "0.20593750, 0.79406250\n",
      "0.20399427, 0.79600573\n",
      "0.20543702, 0.79456298\n",
      "0.20487937, 0.79512063\n",
      "0.20511259, 0.79488741\n",
      "0.20466866, 0.79533134\n",
      "0.20653985, 0.79346015\n",
      "0.20451633, 0.79548367\n",
      "0.20576667, 0.79423333\n",
      "0.20479111, 0.79520889\n",
      "0.20617157, 0.79382843\n",
      "0.20716379, 0.79283621\n",
      "0.20610615, 0.79389385\n",
      "0.20599814, 0.79400186\n",
      "0.20477624, 0.79522376\n",
      "0.20517357, 0.79482643\n",
      "0.20408139, 0.79591861\n",
      "0.20375020, 0.79624980\n",
      "0.20592040, 0.79407960\n",
      "0.20597861, 0.79402139\n",
      "0.20736963, 0.79263037\n",
      "0.20522077, 0.79477923\n",
      "0.20450094, 0.79549906\n",
      "0.20490097, 0.79509903\n",
      "0.20577611, 0.79422389\n",
      "0.20589953, 0.79410047\n",
      "0.20462957, 0.79537043\n",
      "0.20528113, 0.79471887\n",
      "0.20681773, 0.79318227\n",
      "0.20503420, 0.79496580\n",
      "0.20593973, 0.79406027\n",
      "0.20484203, 0.79515797\n",
      "0.20594165, 0.79405835\n",
      "0.20396195, 0.79603805\n",
      "0.20516852, 0.79483148\n",
      "0.20499130, 0.79500870\n",
      "0.20492182, 0.79507818\n",
      "0.20476883, 0.79523117\n",
      "0.20601870, 0.79398130\n",
      "0.20512568, 0.79487432\n",
      "0.20577880, 0.79422120\n",
      "0.20526154, 0.79473846\n",
      "0.20705168, 0.79294832\n",
      "0.20485430, 0.79514570\n",
      "0.20513212, 0.79486788\n",
      "0.20428189, 0.79571811\n",
      "0.20508461, 0.79491539\n",
      "0.20615654, 0.79384346\n",
      "0.20530905, 0.79469095\n",
      "0.20509665, 0.79490335\n",
      "0.20446888, 0.79553112\n",
      "0.20569111, 0.79430889\n",
      "0.20419370, 0.79580630\n",
      "0.20464942, 0.79535058\n",
      "0.20507482, 0.79492518\n",
      "0.20537471, 0.79462529\n",
      "0.20493125, 0.79506875\n",
      "0.20571630, 0.79428370\n",
      "0.20534510, 0.79465490\n",
      "0.20407903, 0.79592097\n",
      "0.20460653, 0.79539347\n",
      "0.20546678, 0.79453322\n",
      "0.20525622, 0.79474378\n",
      "0.20497498, 0.79502502\n",
      "0.20554398, 0.79445602\n",
      "0.20555884, 0.79444116\n",
      "0.20319228, 0.79680772\n",
      "0.20441168, 0.79558832\n",
      "0.20406747, 0.79593253\n",
      "0.20597999, 0.79402001\n",
      "0.20620020, 0.79379980\n",
      "0.20493765, 0.79506235\n",
      "0.20639742, 0.79360258\n",
      "0.20438107, 0.79561893\n",
      "0.20514443, 0.79485557\n",
      "0.20380684, 0.79619316\n",
      "0.20436292, 0.79563708\n",
      "0.20572782, 0.79427218\n",
      "0.20414428, 0.79585572\n",
      "0.20427179, 0.79572821\n",
      "0.20513031, 0.79486969\n",
      "0.20469722, 0.79530278\n",
      "0.20504546, 0.79495454\n",
      "0.20614801, 0.79385199\n",
      "0.20542049, 0.79457951\n",
      "0.20590190, 0.79409810\n",
      "0.20379685, 0.79620315\n",
      "0.20561676, 0.79438324\n",
      "0.20554074, 0.79445926\n",
      "0.20590726, 0.79409274\n",
      "0.20464337, 0.79535663\n",
      "0.20506167, 0.79493833\n",
      "0.20469160, 0.79530840\n",
      "0.20505686, 0.79494314\n",
      "0.20690409, 0.79309591\n",
      "0.20683805, 0.79316195\n",
      "0.20577260, 0.79422740\n",
      "0.20550547, 0.79449453\n",
      "0.20680359, 0.79319641\n",
      "0.20536861, 0.79463139\n",
      "0.20547000, 0.79453000\n",
      "0.20422778, 0.79577222\n",
      "0.20466263, 0.79533737\n",
      "0.20355475, 0.79644525\n",
      "0.20591851, 0.79408149\n",
      "0.20477217, 0.79522783\n",
      "0.20480996, 0.79519004\n",
      "0.20591233, 0.79408767\n",
      "0.20585008, 0.79414992\n",
      "0.20489506, 0.79510494\n",
      "0.20498578, 0.79501422\n",
      "0.20536723, 0.79463277\n",
      "0.20518048, 0.79481952\n",
      "0.20785722, 0.79214278\n",
      "0.20672339, 0.79327661\n",
      "0.20529882, 0.79470118\n",
      "0.20581887, 0.79418113\n",
      "0.20642532, 0.79357468\n",
      "0.20397620, 0.79602380\n",
      "0.20502877, 0.79497123\n",
      "0.20466955, 0.79533045\n",
      "0.20659663, 0.79340337\n",
      "0.20477218, 0.79522782\n",
      "0.20577882, 0.79422118\n",
      "0.20532882, 0.79467118\n",
      "0.20480370, 0.79519630\n",
      "0.20598399, 0.79401601\n",
      "0.20477920, 0.79522080\n",
      "0.20603424, 0.79396576\n",
      "0.20518776, 0.79481224\n",
      "0.20619268, 0.79380732\n",
      "0.20419626, 0.79580374\n",
      "0.20466022, 0.79533978\n",
      "0.20599130, 0.79400870\n",
      "0.20432658, 0.79567342\n",
      "0.20609693, 0.79390307\n",
      "0.20580455, 0.79419545\n",
      "0.20583933, 0.79416067\n",
      "0.20825539, 0.79174461\n",
      "0.20525576, 0.79474424\n",
      "0.20494760, 0.79505240\n",
      "0.20631651, 0.79368349\n",
      "0.20615493, 0.79384507\n",
      "0.20495073, 0.79504927\n",
      "0.20557440, 0.79442560\n",
      "0.20565150, 0.79434850\n",
      "0.20761027, 0.79238973\n",
      "0.20638197, 0.79361803\n",
      "0.20560219, 0.79439781\n",
      "0.20488998, 0.79511002\n",
      "0.20611348, 0.79388652\n",
      "0.20497445, 0.79502555\n",
      "0.20508432, 0.79491568\n",
      "0.20487267, 0.79512733\n",
      "0.20503925, 0.79496075\n",
      "0.20612427, 0.79387573\n",
      "0.20570728, 0.79429272\n",
      "0.20575054, 0.79424946\n",
      "0.20602574, 0.79397426\n",
      "0.20520777, 0.79479223\n",
      "0.20714942, 0.79285058\n",
      "0.20600035, 0.79399965\n",
      "0.20462050, 0.79537950\n",
      "0.20645656, 0.79354344\n",
      "0.20526862, 0.79473138\n",
      "0.20495763, 0.79504237\n",
      "0.20719117, 0.79280883\n",
      "0.20577880, 0.79422120\n",
      "0.20388989, 0.79611011\n",
      "0.20468067, 0.79531933\n",
      "0.20523055, 0.79476945\n",
      "0.20483442, 0.79516558\n",
      "0.20775825, 0.79224175\n",
      "0.20522670, 0.79477330\n",
      "0.20549059, 0.79450941\n",
      "0.20581100, 0.79418900\n",
      "0.20633173, 0.79366827\n",
      "0.20614478, 0.79385522\n",
      "0.20586833, 0.79413167\n",
      "0.20622995, 0.79377005\n",
      "0.20369467, 0.79630533\n",
      "0.20422595, 0.79577405\n",
      "0.20688323, 0.79311677\n",
      "0.20485153, 0.79514847\n",
      "0.20562333, 0.79437667\n",
      "0.20566770, 0.79433230\n",
      "0.20577178, 0.79422822\n",
      "0.20511589, 0.79488411\n",
      "0.20656103, 0.79343897\n",
      "0.20518533, 0.79481467\n",
      "0.20728994, 0.79271006\n",
      "0.20522481, 0.79477519\n",
      "0.20425633, 0.79574367\n",
      "0.20408474, 0.79591526\n",
      "0.20429032, 0.79570968\n",
      "0.20613998, 0.79386002\n",
      "0.20585904, 0.79414096\n",
      "0.20573880, 0.79426120\n",
      "0.20439731, 0.79560269\n",
      "0.20554292, 0.79445708\n",
      "0.20535439, 0.79464561\n",
      "0.20436338, 0.79563662\n",
      "0.20568588, 0.79431412\n",
      "0.20576223, 0.79423777\n",
      "0.20633440, 0.79366560\n",
      "0.20640515, 0.79359485\n",
      "0.20695848, 0.79304152\n",
      "0.20537823, 0.79462177\n",
      "0.20550616, 0.79449384\n",
      "0.20542773, 0.79457227\n",
      "0.20463019, 0.79536981\n",
      "0.20305196, 0.79694804\n",
      "0.20660054, 0.79339946\n",
      "0.20505557, 0.79494443\n",
      "0.20520443, 0.79479557\n",
      "0.20621304, 0.79378696\n",
      "0.20560497, 0.79439503\n",
      "0.20590888, 0.79409112\n",
      "0.20458265, 0.79541735\n",
      "0.20501773, 0.79498227\n",
      "0.20455782, 0.79544218\n",
      "0.20468462, 0.79531538\n",
      "0.20603244, 0.79396756\n",
      "0.20679889, 0.79320111\n",
      "0.20445260, 0.79554740\n",
      "0.20456011, 0.79543989\n",
      "0.20460931, 0.79539069\n",
      "0.20533565, 0.79466435\n",
      "0.20511957, 0.79488043\n",
      "0.20530699, 0.79469301\n",
      "0.20422173, 0.79577827\n",
      "0.20578323, 0.79421677\n",
      "0.20433548, 0.79566452\n",
      "0.20528198, 0.79471802\n",
      "0.20474837, 0.79525163\n",
      "0.20327375, 0.79672625\n",
      "0.20447140, 0.79552860\n",
      "0.20666431, 0.79333569\n",
      "0.20567841, 0.79432159\n",
      "0.20453414, 0.79546586\n",
      "0.20493978, 0.79506022\n",
      "0.20697975, 0.79302025\n",
      "0.20485461, 0.79514539\n",
      "0.20452840, 0.79547160\n",
      "0.20440494, 0.79559506\n",
      "0.20521446, 0.79478554\n",
      "0.20498861, 0.79501139\n",
      "0.20531165, 0.79468835\n",
      "0.20596707, 0.79403293\n",
      "0.20499336, 0.79500664\n",
      "0.20531551, 0.79468449\n",
      "0.20533889, 0.79466111\n",
      "0.20366463, 0.79633537\n",
      "0.20402320, 0.79597680\n",
      "0.20556689, 0.79443311\n",
      "0.20576369, 0.79423631\n",
      "0.20526597, 0.79473403\n",
      "0.20462973, 0.79537027\n",
      "0.20560995, 0.79439005\n",
      "0.20581445, 0.79418555\n",
      "0.20629910, 0.79370090\n",
      "0.20522877, 0.79477123\n",
      "0.20551420, 0.79448580\n",
      "0.20515221, 0.79484779\n",
      "0.20512271, 0.79487729\n",
      "0.20694216, 0.79305784\n",
      "0.20384692, 0.79615308\n",
      "0.20894746, 0.79105254\n",
      "0.20436807, 0.79563193\n",
      "0.20488844, 0.79511156\n",
      "0.20425257, 0.79574743\n",
      "0.20649156, 0.79350844\n",
      "0.20482799, 0.79517201\n",
      "0.20482892, 0.79517108\n",
      "0.20501294, 0.79498706\n",
      "0.20672116, 0.79327884\n",
      "0.20544831, 0.79455169\n",
      "0.20492652, 0.79507348\n",
      "0.20488487, 0.79511513\n",
      "0.20552781, 0.79447219\n",
      "0.20519619, 0.79480381\n",
      "0.20593473, 0.79406527\n",
      "0.20499768, 0.79500232\n",
      "0.20506372, 0.79493628\n",
      "0.20587292, 0.79412708\n",
      "0.20422067, 0.79577933\n",
      "0.20330892, 0.79669108\n",
      "0.20426084, 0.79573916\n",
      "0.20523431, 0.79476569\n",
      "0.20569022, 0.79430978\n",
      "0.20441180, 0.79558820\n",
      "0.20608710, 0.79391290\n",
      "0.20484350, 0.79515650\n",
      "0.20517996, 0.79482004\n",
      "0.20429789, 0.79570211\n",
      "0.20542645, 0.79457355\n",
      "0.20565792, 0.79434208\n",
      "0.20657870, 0.79342130\n",
      "0.20541035, 0.79458965\n",
      "0.20501809, 0.79498191\n",
      "0.20632640, 0.79367360\n",
      "0.20478335, 0.79521665\n",
      "0.20645134, 0.79354866\n",
      "0.20632919, 0.79367081\n",
      "0.20596416, 0.79403584\n",
      "0.20696704, 0.79303296\n",
      "0.20660482, 0.79339518\n",
      "0.20613055, 0.79386945\n",
      "0.20478808, 0.79521192\n",
      "0.20414362, 0.79585638\n",
      "0.20698619, 0.79301381\n",
      "0.20574617, 0.79425383\n",
      "0.20487552, 0.79512448\n",
      "0.20475291, 0.79524709\n",
      "0.20514698, 0.79485302\n",
      "0.20630174, 0.79369826\n",
      "0.20611730, 0.79388270\n",
      "0.20563238, 0.79436762\n",
      "0.20528070, 0.79471930\n",
      "0.20580449, 0.79419551\n",
      "0.20517632, 0.79482368\n",
      "0.20396915, 0.79603085\n",
      "0.20527970, 0.79472030\n",
      "0.20459651, 0.79540349\n",
      "0.20599223, 0.79400777\n",
      "0.20531367, 0.79468633\n",
      "0.20566611, 0.79433389\n",
      "0.20538415, 0.79461585\n",
      "0.20512385, 0.79487615\n",
      "0.20646903, 0.79353097\n",
      "0.20551977, 0.79448023\n",
      "0.20616642, 0.79383358\n",
      "0.20464891, 0.79535109\n",
      "0.20538668, 0.79461332\n",
      "0.20598324, 0.79401676\n",
      "0.20418702, 0.79581298\n",
      "0.20664457, 0.79335543\n",
      "0.20474035, 0.79525965\n",
      "0.20468680, 0.79531320\n",
      "0.20452855, 0.79547145\n",
      "0.20540235, 0.79459765\n",
      "0.20368782, 0.79631218\n",
      "0.20546377, 0.79453623\n",
      "0.20478157, 0.79521843\n",
      "0.20624792, 0.79375208\n",
      "0.20600387, 0.79399613\n",
      "0.20615455, 0.79384545\n",
      "0.20553412, 0.79446588\n",
      "0.20484060, 0.79515940\n",
      "0.20440109, 0.79559891\n",
      "0.20449174, 0.79550826\n",
      "0.20421904, 0.79578096\n",
      "0.20510981, 0.79489019\n",
      "0.20569969, 0.79430031\n",
      "0.20527506, 0.79472494\n",
      "0.20651659, 0.79348341\n",
      "0.20463187, 0.79536813\n",
      "0.20453791, 0.79546209\n",
      "0.20499679, 0.79500321\n",
      "0.20604875, 0.79395125\n",
      "0.20517405, 0.79482595\n",
      "0.20619165, 0.79380835\n",
      "0.20469391, 0.79530609\n",
      "0.20508697, 0.79491303\n",
      "0.20497240, 0.79502760\n",
      "0.20495977, 0.79504023\n",
      "0.20580447, 0.79419553\n",
      "0.20565174, 0.79434826\n",
      "0.20475568, 0.79524432\n",
      "0.20580314, 0.79419686\n",
      "0.20405704, 0.79594296\n",
      "0.20565847, 0.79434153\n",
      "0.20625259, 0.79374741\n",
      "0.20605535, 0.79394465\n",
      "0.20573124, 0.79426876\n",
      "0.20455396, 0.79544604\n",
      "0.20529400, 0.79470600\n",
      "0.20556175, 0.79443825\n",
      "0.20562804, 0.79437196\n",
      "0.20403238, 0.79596762\n",
      "0.20303881, 0.79696119\n",
      "0.20541753, 0.79458247\n",
      "0.20544544, 0.79455456\n",
      "0.20565592, 0.79434408\n",
      "0.20556394, 0.79443606\n",
      "0.20479182, 0.79520818\n",
      "0.20481897, 0.79518103\n",
      "0.20529812, 0.79470188\n",
      "0.20618900, 0.79381100\n",
      "0.20601797, 0.79398203\n",
      "0.20464831, 0.79535169\n",
      "0.20559492, 0.79440508\n",
      "0.20527469, 0.79472531\n",
      "0.20668569, 0.79331431\n",
      "0.20547080, 0.79452920\n",
      "0.20455653, 0.79544347\n",
      "0.20606456, 0.79393544\n",
      "0.20480878, 0.79519122\n",
      "0.20568317, 0.79431683\n",
      "0.20501426, 0.79498574\n",
      "0.20591666, 0.79408334\n",
      "0.20477399, 0.79522601\n",
      "0.20516223, 0.79483777\n",
      "0.20576485, 0.79423515\n",
      "0.20375547, 0.79624453\n",
      "0.20519096, 0.79480904\n",
      "0.20533883, 0.79466117\n",
      "0.20455091, 0.79544909\n",
      "0.20385002, 0.79614998\n",
      "0.20461364, 0.79538636\n",
      "0.20446210, 0.79553790\n",
      "0.20490326, 0.79509674\n",
      "0.20586603, 0.79413397\n",
      "0.20512200, 0.79487800\n",
      "0.20440229, 0.79559771\n",
      "0.20562104, 0.79437896\n",
      "0.20568798, 0.79431202\n",
      "0.20588915, 0.79411085\n",
      "0.20655927, 0.79344073\n",
      "0.20539957, 0.79460043\n",
      "0.20542807, 0.79457193\n",
      "0.20453064, 0.79546936\n",
      "0.20441442, 0.79558558\n",
      "0.20516043, 0.79483957\n",
      "0.20478740, 0.79521260\n",
      "0.20364818, 0.79635182\n",
      "0.20643932, 0.79356068\n",
      "0.20610997, 0.79389003\n",
      "0.20782881, 0.79217119\n",
      "0.20501389, 0.79498611\n",
      "0.20663243, 0.79336757\n",
      "0.20592130, 0.79407870\n",
      "0.20628625, 0.79371375\n",
      "0.20408681, 0.79591319\n",
      "0.20463733, 0.79536267\n",
      "0.20407472, 0.79592528\n",
      "0.20606847, 0.79393153\n",
      "0.20540927, 0.79459073\n",
      "0.20603299, 0.79396701\n",
      "0.20465907, 0.79534093\n",
      "0.20429809, 0.79570191\n",
      "0.20645635, 0.79354365\n",
      "0.20519281, 0.79480719\n",
      "0.20413390, 0.79586610\n",
      "0.20279822, 0.79720178\n",
      "0.20503369, 0.79496631\n",
      "0.20446272, 0.79553728\n",
      "0.20458050, 0.79541950\n",
      "0.20594006, 0.79405994\n",
      "0.20592115, 0.79407885\n",
      "0.20426312, 0.79573688\n",
      "0.20555241, 0.79444759\n",
      "0.20570138, 0.79429862\n",
      "0.20510945, 0.79489055\n",
      "0.20572074, 0.79427926\n",
      "0.20418403, 0.79581597\n",
      "0.20497113, 0.79502887\n",
      "0.20420571, 0.79579429\n",
      "0.20468803, 0.79531197\n",
      "0.20436302, 0.79563698\n",
      "0.20633796, 0.79366204\n",
      "0.20563534, 0.79436466\n",
      "0.20556976, 0.79443024\n",
      "0.20534548, 0.79465452\n",
      "0.20617661, 0.79382339\n",
      "0.20665336, 0.79334664\n",
      "0.20618814, 0.79381186\n",
      "0.20439352, 0.79560648\n",
      "0.20471005, 0.79528995\n",
      "0.20565079, 0.79434921\n",
      "0.20515753, 0.79484247\n",
      "0.20491991, 0.79508009\n",
      "0.20502497, 0.79497503\n",
      "0.20544107, 0.79455893\n",
      "0.20580075, 0.79419925\n",
      "0.20596136, 0.79403864\n",
      "0.20499762, 0.79500238\n",
      "0.20646066, 0.79353934\n",
      "0.20421711, 0.79578289\n",
      "0.20366816, 0.79633184\n",
      "0.20593563, 0.79406437\n",
      "0.20466132, 0.79533868\n",
      "0.20523135, 0.79476865\n",
      "0.20512872, 0.79487128\n",
      "0.20515372, 0.79484628\n",
      "0.20475260, 0.79524740\n",
      "0.20443200, 0.79556800\n",
      "0.20573647, 0.79426353\n",
      "0.20575136, 0.79424864\n",
      "0.20620829, 0.79379171\n",
      "0.20438615, 0.79561385\n",
      "0.20542757, 0.79457243\n",
      "0.20492082, 0.79507918\n",
      "0.20638527, 0.79361473\n",
      "0.20567125, 0.79432875\n",
      "0.20449799, 0.79550201\n",
      "0.20572104, 0.79427896\n",
      "0.20512722, 0.79487278\n",
      "0.20553909, 0.79446091\n",
      "0.20534462, 0.79465538\n",
      "0.20445236, 0.79554764\n",
      "0.20550633, 0.79449367\n",
      "0.20428803, 0.79571197\n",
      "0.20598392, 0.79401608\n",
      "0.20544115, 0.79455885\n",
      "0.20634124, 0.79365876\n",
      "0.20552156, 0.79447844\n",
      "0.20561585, 0.79438415\n",
      "0.20496694, 0.79503306\n",
      "0.20625106, 0.79374894\n",
      "0.20474687, 0.79525313\n",
      "0.20391585, 0.79608415\n",
      "0.20517942, 0.79482058\n",
      "0.20527248, 0.79472752\n",
      "0.20511922, 0.79488078\n",
      "0.20528104, 0.79471896\n",
      "0.20614778, 0.79385222\n",
      "0.20460220, 0.79539780\n",
      "0.20435666, 0.79564334\n",
      "0.20671653, 0.79328347\n",
      "0.20651034, 0.79348966\n",
      "0.20474042, 0.79525958\n",
      "0.20618819, 0.79381181\n",
      "0.20549626, 0.79450374\n",
      "0.20575880, 0.79424120\n",
      "0.20526352, 0.79473648\n",
      "0.20717610, 0.79282390\n",
      "0.20499930, 0.79500070\n",
      "0.20597147, 0.79402853\n",
      "0.20492220, 0.79507780\n",
      "0.20300856, 0.79699144\n",
      "0.20662369, 0.79337631\n",
      "0.20356464, 0.79643536\n",
      "0.20408507, 0.79591493\n",
      "0.20540930, 0.79459070\n",
      "0.20554401, 0.79445599\n",
      "0.20428077, 0.79571923\n",
      "0.20593952, 0.79406048\n",
      "0.20555067, 0.79444933\n",
      "0.20648101, 0.79351899\n",
      "0.20519332, 0.79480668\n",
      "0.20718584, 0.79281416\n",
      "0.20448175, 0.79551825\n",
      "0.20506648, 0.79493352\n",
      "0.20660758, 0.79339242\n",
      "0.20528217, 0.79471783\n",
      "0.20605818, 0.79394182\n",
      "0.20583338, 0.79416662\n",
      "0.20396547, 0.79603453\n",
      "0.20563231, 0.79436769\n",
      "0.20540842, 0.79459158\n",
      "0.20368725, 0.79631275\n",
      "0.20584747, 0.79415253\n",
      "0.20559732, 0.79440268\n",
      "0.20530420, 0.79469580\n",
      "0.20660249, 0.79339751\n",
      "0.20556319, 0.79443681\n",
      "0.20499337, 0.79500663\n",
      "0.20400098, 0.79599902\n",
      "0.20541398, 0.79458602\n",
      "0.20543537, 0.79456463\n",
      "0.20514167, 0.79485833\n",
      "0.20535104, 0.79464896\n",
      "0.20464173, 0.79535827\n",
      "0.20443686, 0.79556314\n",
      "0.20535090, 0.79464910\n",
      "0.20516690, 0.79483310\n",
      "0.20531662, 0.79468338\n",
      "0.20559636, 0.79440364\n",
      "0.20487568, 0.79512432\n",
      "0.20531513, 0.79468487\n",
      "0.20568383, 0.79431617\n",
      "0.20726425, 0.79273575\n",
      "0.20426032, 0.79573968\n",
      "0.20645751, 0.79354249\n",
      "0.20438527, 0.79561473\n",
      "0.20601122, 0.79398878\n",
      "0.20542883, 0.79457117\n",
      "0.20326862, 0.79673138\n",
      "0.20598052, 0.79401948\n",
      "0.20514030, 0.79485970\n",
      "0.20621458, 0.79378542\n",
      "0.20460181, 0.79539819\n",
      "0.20350094, 0.79649906\n",
      "0.20564900, 0.79435100\n",
      "0.20701858, 0.79298142\n",
      "0.20554338, 0.79445662\n",
      "0.20609996, 0.79390004\n",
      "0.20597578, 0.79402422\n",
      "0.20607581, 0.79392419\n",
      "0.20614942, 0.79385058\n",
      "0.20569133, 0.79430867\n",
      "0.20365864, 0.79634136\n",
      "0.20538778, 0.79461222\n",
      "0.20418513, 0.79581487\n",
      "0.20478262, 0.79521738\n",
      "0.20530751, 0.79469249\n",
      "0.20465166, 0.79534834\n",
      "0.20628181, 0.79371819\n",
      "0.20468025, 0.79531975\n",
      "0.20562228, 0.79437772\n",
      "0.20612732, 0.79387268\n",
      "0.20483081, 0.79516919\n",
      "0.20661559, 0.79338441\n",
      "0.20587485, 0.79412515\n",
      "0.20469709, 0.79530291\n",
      "0.20511963, 0.79488037\n",
      "0.20519427, 0.79480573\n",
      "0.20420961, 0.79579039\n",
      "0.20414833, 0.79585167\n",
      "0.20595530, 0.79404470\n",
      "0.20534117, 0.79465883\n",
      "0.20476233, 0.79523767\n",
      "0.20672260, 0.79327740\n",
      "0.20419898, 0.79580102\n",
      "0.20701424, 0.79298576\n",
      "0.20855400, 0.79144600\n",
      "0.20619648, 0.79380352\n",
      "0.20628218, 0.79371782\n",
      "0.20476137, 0.79523863\n",
      "0.20670002, 0.79329998\n",
      "0.20542546, 0.79457454\n",
      "0.20487652, 0.79512348\n",
      "0.20395657, 0.79604343\n",
      "0.20433748, 0.79566252\n",
      "0.20479858, 0.79520142\n",
      "0.20666524, 0.79333476\n",
      "0.20544337, 0.79455663\n",
      "0.20415430, 0.79584570\n",
      "0.20504892, 0.79495108\n",
      "0.20613143, 0.79386857\n",
      "0.20414238, 0.79585762\n",
      "0.20596890, 0.79403110\n",
      "0.20507569, 0.79492431\n",
      "0.20431315, 0.79568685\n",
      "0.20530429, 0.79469571\n",
      "0.20461805, 0.79538195\n",
      "0.20415799, 0.79584201\n",
      "0.20439010, 0.79560990\n",
      "0.20578890, 0.79421110\n",
      "0.20492568, 0.79507432\n",
      "0.20496592, 0.79503408\n",
      "0.20446793, 0.79553207\n",
      "0.20595651, 0.79404349\n",
      "0.20540574, 0.79459426\n",
      "0.20316923, 0.79683077\n",
      "0.20597917, 0.79402083\n",
      "0.20523307, 0.79476693\n",
      "0.20514453, 0.79485547\n",
      "0.20357745, 0.79642255\n",
      "0.20575491, 0.79424509\n",
      "0.20531638, 0.79468362\n",
      "0.20540329, 0.79459671\n",
      "0.20546710, 0.79453290\n",
      "0.20478877, 0.79521123\n",
      "0.20473972, 0.79526028\n",
      "0.20669891, 0.79330109\n",
      "0.20601130, 0.79398870\n",
      "0.20609150, 0.79390850\n",
      "0.20712178, 0.79287822\n",
      "0.20571980, 0.79428020\n",
      "0.20529271, 0.79470729\n",
      "0.20512627, 0.79487373\n",
      "0.20528227, 0.79471773\n",
      "0.20498208, 0.79501792\n",
      "0.20510852, 0.79489148\n",
      "0.20352924, 0.79647076\n",
      "0.20377657, 0.79622343\n",
      "0.20386148, 0.79613852\n",
      "0.20556327, 0.79443673\n",
      "0.20540848, 0.79459152\n",
      "0.20585801, 0.79414199\n",
      "0.20583527, 0.79416473\n",
      "0.20448824, 0.79551176\n",
      "0.20574004, 0.79425996\n",
      "0.20461570, 0.79538430\n",
      "0.20405423, 0.79594577\n",
      "0.20448472, 0.79551528\n",
      "0.20425789, 0.79574211\n",
      "0.20502403, 0.79497597\n",
      "0.20624643, 0.79375357\n",
      "0.20624301, 0.79375699\n",
      "0.20474027, 0.79525973\n",
      "0.20436628, 0.79563372\n",
      "0.20652218, 0.79347782\n",
      "0.20693147, 0.79306853\n",
      "0.20557966, 0.79442034\n",
      "0.20550343, 0.79449657\n",
      "0.20530238, 0.79469762\n",
      "0.20439114, 0.79560886\n",
      "0.20518743, 0.79481257\n",
      "0.20486326, 0.79513674\n",
      "0.20468704, 0.79531296\n",
      "0.20467534, 0.79532466\n",
      "0.20532498, 0.79467502\n",
      "0.20436353, 0.79563647\n",
      "0.20495735, 0.79504265\n",
      "0.20466084, 0.79533916\n",
      "0.20499585, 0.79500415\n",
      "0.20550095, 0.79449905\n",
      "0.20555463, 0.79444537\n",
      "0.20847413, 0.79152587\n",
      "0.20445690, 0.79554310\n",
      "0.20505748, 0.79494252\n",
      "0.20533243, 0.79466757\n",
      "0.20496587, 0.79503413\n",
      "0.20490501, 0.79509499\n",
      "0.20851873, 0.79148127\n",
      "0.20482211, 0.79517789\n",
      "0.20457702, 0.79542298\n",
      "0.20509265, 0.79490735\n",
      "0.20517360, 0.79482640\n",
      "0.20325610, 0.79674390\n",
      "0.20638814, 0.79361186\n",
      "0.20498598, 0.79501402\n",
      "0.20508218, 0.79491782\n",
      "0.20417977, 0.79582023\n",
      "0.20545851, 0.79454149\n",
      "0.20555592, 0.79444408\n",
      "0.20481425, 0.79518575\n",
      "0.20665517, 0.79334483\n",
      "0.20554700, 0.79445300\n",
      "0.20356929, 0.79643071\n",
      "0.20419075, 0.79580925\n",
      "0.20561209, 0.79438791\n",
      "0.20620659, 0.79379341\n",
      "0.20480064, 0.79519936\n",
      "0.20415450, 0.79584550\n",
      "0.20561551, 0.79438449\n",
      "0.20452620, 0.79547380\n",
      "0.20615728, 0.79384272\n",
      "0.20460008, 0.79539992\n",
      "0.20538787, 0.79461213\n",
      "0.20320580, 0.79679420\n",
      "0.20567695, 0.79432305\n",
      "0.20462246, 0.79537754\n",
      "0.20648920, 0.79351080\n",
      "0.20489880, 0.79510120\n",
      "0.20803135, 0.79196865\n",
      "0.20486448, 0.79513552\n",
      "0.20448040, 0.79551960\n",
      "0.20498836, 0.79501164\n",
      "0.20422523, 0.79577477\n",
      "0.20541892, 0.79458108\n",
      "0.20497943, 0.79502057\n",
      "0.20622178, 0.79377822\n",
      "0.20431280, 0.79568720\n",
      "0.20472433, 0.79527567\n",
      "0.20637913, 0.79362087\n",
      "0.20519451, 0.79480549\n",
      "0.20528197, 0.79471803\n",
      "0.20558355, 0.79441645\n",
      "0.20570969, 0.79429031\n",
      "0.20578526, 0.79421474\n",
      "0.20516912, 0.79483088\n",
      "0.20560858, 0.79439142\n",
      "0.20447123, 0.79552877\n",
      "0.20539494, 0.79460506\n",
      "0.20498111, 0.79501889\n",
      "0.20585231, 0.79414769\n",
      "0.20427237, 0.79572763\n",
      "0.20548764, 0.79451236\n",
      "0.20575656, 0.79424344\n",
      "0.20526766, 0.79473234\n",
      "0.20592636, 0.79407364\n",
      "0.20500261, 0.79499739\n",
      "0.20276710, 0.79723290\n",
      "0.20459289, 0.79540711\n",
      "0.20481068, 0.79518932\n",
      "0.20476752, 0.79523248\n",
      "0.20661072, 0.79338928\n",
      "0.20575725, 0.79424275\n",
      "0.20604197, 0.79395803\n",
      "0.20516329, 0.79483671\n",
      "0.20583721, 0.79416279\n",
      "0.20470025, 0.79529975\n",
      "0.20383024, 0.79616976\n",
      "0.20535476, 0.79464524\n",
      "0.20436210, 0.79563790\n",
      "0.20475123, 0.79524877\n",
      "0.20466360, 0.79533640\n",
      "0.20672548, 0.79327452\n",
      "0.20406172, 0.79593828\n",
      "0.20538568, 0.79461432\n",
      "0.20565728, 0.79434272\n",
      "0.20417564, 0.79582436\n",
      "0.20636060, 0.79363940\n",
      "0.20558911, 0.79441089\n",
      "0.20534119, 0.79465881\n",
      "0.20426536, 0.79573464\n",
      "0.20507622, 0.79492378\n",
      "0.20439909, 0.79560091\n",
      "0.20441680, 0.79558320\n",
      "0.20642106, 0.79357894\n",
      "0.20430838, 0.79569162\n",
      "0.20536712, 0.79463288\n",
      "0.20505922, 0.79494078\n",
      "0.20603695, 0.79396305\n",
      "0.20455134, 0.79544866\n",
      "0.20471914, 0.79528086\n",
      "0.20457033, 0.79542967\n",
      "0.20587203, 0.79412797\n",
      "0.20507318, 0.79492682\n",
      "0.20636675, 0.79363325\n",
      "0.20646886, 0.79353114\n",
      "0.20489687, 0.79510313\n",
      "0.20517026, 0.79482974\n",
      "0.20569416, 0.79430584\n",
      "0.20593026, 0.79406974\n",
      "0.20527181, 0.79472819\n",
      "0.20434258, 0.79565742\n",
      "0.20466848, 0.79533152\n",
      "0.20535789, 0.79464211\n",
      "0.20589031, 0.79410969\n",
      "0.20403768, 0.79596232\n",
      "0.20563933, 0.79436067\n",
      "0.20435749, 0.79564251\n",
      "0.20450439, 0.79549561\n",
      "0.20522645, 0.79477355\n",
      "0.20370598, 0.79629402\n",
      "0.20688001, 0.79311999\n",
      "0.20451154, 0.79548846\n",
      "0.20511138, 0.79488862\n",
      "0.20529107, 0.79470893\n",
      "0.20511365, 0.79488635\n",
      "0.20525819, 0.79474181\n",
      "0.20497523, 0.79502477\n",
      "0.20650916, 0.79349084\n",
      "0.20360072, 0.79639928\n",
      "0.20493608, 0.79506392\n",
      "0.20519081, 0.79480919\n",
      "0.20525834, 0.79474166\n",
      "0.20509042, 0.79490958\n",
      "0.20357504, 0.79642496\n",
      "0.21054879, 0.78945121\n",
      "0.20546110, 0.79453890\n",
      "0.20498032, 0.79501968\n",
      "0.20649374, 0.79350626\n",
      "0.20513438, 0.79486562\n",
      "0.20363556, 0.79636444\n",
      "0.20522434, 0.79477566\n",
      "0.20521172, 0.79478828\n",
      "0.20634939, 0.79365061\n",
      "0.20541705, 0.79458295\n",
      "0.20657358, 0.79342642\n",
      "0.20491351, 0.79508649\n",
      "0.20611500, 0.79388500\n",
      "0.20691684, 0.79308316\n",
      "0.20430429, 0.79569571\n",
      "0.20708306, 0.79291694\n",
      "0.20609636, 0.79390364\n",
      "0.20610888, 0.79389112\n",
      "0.20424594, 0.79575406\n",
      "0.20432923, 0.79567077\n",
      "0.20556431, 0.79443569\n",
      "0.20385466, 0.79614534\n",
      "0.20513033, 0.79486967\n",
      "0.20585623, 0.79414377\n",
      "0.20458355, 0.79541645\n",
      "0.20619777, 0.79380223\n",
      "0.20442092, 0.79557908\n",
      "0.20556965, 0.79443035\n",
      "0.20679397, 0.79320603\n",
      "0.20571043, 0.79428957\n",
      "0.20334550, 0.79665450\n",
      "0.20487919, 0.79512081\n",
      "0.20414684, 0.79585316\n",
      "0.20512374, 0.79487626\n",
      "0.20573337, 0.79426663\n",
      "0.20881385, 0.79118615\n",
      "0.20471245, 0.79528755\n",
      "0.20400624, 0.79599376\n",
      "0.20484765, 0.79515235\n",
      "0.20496650, 0.79503350\n",
      "0.20633271, 0.79366729\n",
      "0.20600530, 0.79399470\n",
      "0.20559227, 0.79440773\n",
      "0.20445803, 0.79554197\n",
      "0.20638854, 0.79361146\n",
      "0.20405408, 0.79594592\n",
      "0.20625688, 0.79374312\n",
      "0.20492230, 0.79507770\n",
      "0.20643246, 0.79356754\n",
      "0.20416724, 0.79583276\n",
      "0.20674085, 0.79325915\n",
      "0.20454488, 0.79545512\n",
      "0.20748164, 0.79251836\n",
      "0.20504771, 0.79495229\n",
      "0.20606004, 0.79393996\n",
      "0.20499285, 0.79500715\n",
      "0.20534999, 0.79465001\n",
      "0.20345421, 0.79654579\n",
      "0.20584026, 0.79415974\n",
      "0.20499677, 0.79500323\n",
      "0.20608294, 0.79391706\n",
      "0.20475765, 0.79524235\n",
      "0.20523796, 0.79476204\n",
      "0.20410443, 0.79589557\n",
      "0.20616571, 0.79383429\n",
      "0.20564807, 0.79435193\n",
      "0.20464083, 0.79535917\n",
      "0.20586666, 0.79413334\n",
      "0.20414896, 0.79585104\n",
      "0.20541159, 0.79458841\n",
      "0.20661757, 0.79338243\n",
      "0.20671999, 0.79328001\n",
      "0.20538527, 0.79461473\n",
      "0.20618605, 0.79381395\n",
      "0.20370260, 0.79629740\n",
      "0.20716791, 0.79283209\n",
      "0.20533715, 0.79466285\n",
      "0.20679125, 0.79320875\n",
      "0.20591328, 0.79408672\n",
      "0.20428607, 0.79571393\n",
      "0.20512333, 0.79487667\n",
      "0.20584457, 0.79415543\n",
      "0.20496157, 0.79503843\n",
      "0.20652089, 0.79347911\n",
      "0.20615263, 0.79384737\n",
      "0.20479749, 0.79520251\n",
      "0.20648337, 0.79351663\n",
      "0.20573339, 0.79426661\n",
      "0.20511231, 0.79488769\n",
      "0.20524987, 0.79475013\n",
      "0.20477750, 0.79522250\n",
      "0.20497646, 0.79502354\n",
      "0.20534680, 0.79465320\n",
      "0.20575336, 0.79424664\n",
      "0.20481516, 0.79518484\n",
      "0.20442785, 0.79557215\n",
      "0.20477471, 0.79522529\n",
      "0.20436472, 0.79563528\n",
      "0.20501225, 0.79498775\n",
      "0.20396838, 0.79603162\n",
      "0.20568602, 0.79431398\n",
      "0.20493119, 0.79506881\n",
      "0.20542087, 0.79457913\n",
      "0.20504747, 0.79495253\n",
      "0.20538018, 0.79461982\n",
      "0.20455822, 0.79544178\n",
      "0.20546292, 0.79453708\n",
      "0.20531953, 0.79468047\n",
      "0.20475972, 0.79524028\n",
      "0.20537629, 0.79462371\n",
      "0.20704423, 0.79295577\n",
      "0.20474401, 0.79525599\n",
      "0.20698433, 0.79301567\n",
      "0.20525202, 0.79474798\n",
      "0.20472476, 0.79527524\n",
      "0.20500866, 0.79499134\n",
      "0.20436113, 0.79563887\n",
      "0.20497681, 0.79502319\n",
      "0.20502584, 0.79497416\n",
      "0.20521257, 0.79478743\n",
      "0.20680121, 0.79319879\n",
      "0.20469683, 0.79530317\n",
      "0.20501612, 0.79498388\n",
      "0.20528474, 0.79471526\n",
      "0.20498886, 0.79501114\n",
      "0.20541116, 0.79458884\n",
      "0.20540519, 0.79459481\n",
      "0.20633408, 0.79366592\n",
      "0.20534222, 0.79465778\n",
      "0.20505306, 0.79494694\n",
      "0.20725645, 0.79274355\n",
      "0.20528765, 0.79471235\n",
      "0.20562248, 0.79437752\n",
      "0.20561736, 0.79438264\n",
      "0.20319660, 0.79680340\n",
      "0.20498881, 0.79501119\n",
      "0.20507844, 0.79492156\n",
      "0.20456641, 0.79543359\n",
      "0.20595127, 0.79404873\n",
      "0.20458385, 0.79541615\n",
      "0.20607523, 0.79392477\n",
      "0.20489795, 0.79510205\n",
      "0.20506901, 0.79493099\n",
      "0.20498569, 0.79501431\n",
      "0.20519847, 0.79480153\n",
      "0.20494822, 0.79505178\n",
      "0.20535203, 0.79464797\n",
      "0.20403731, 0.79596269\n",
      "0.20673523, 0.79326477\n",
      "0.20543704, 0.79456296\n",
      "0.20578491, 0.79421509\n",
      "0.20543560, 0.79456440\n",
      "0.20597307, 0.79402693\n",
      "0.20548337, 0.79451663\n",
      "0.20553994, 0.79446006\n",
      "0.20512276, 0.79487724\n"
     ]
    }
   ],
   "source": [
    "#ANSWER_PROB#\n",
    "# Run this cell when you are ready to submit your test-set probabilities. This cell will generate some\n",
    "# warning messages if something is not right: make sure to address them!\n",
    "if pred_probabilities.shape != (1114, 2):\n",
    "    print('Array is of incorrect shape. Rectify this before submitting.')\n",
    "elif (pred_probabilities.sum(axis=1) != 1.0).all():\n",
    "    print('Submitted values are not correct probabilities. Rectify this before submitting.')\n",
    "else:\n",
    "    for _prob in pred_probabilities:\n",
    "        print('{:.8f}, {:.8f}'.format(_prob[0], _prob[1]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
